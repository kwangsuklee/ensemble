{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset_train_1.shape ==  (245760, 1)\n",
      " dataset_train_2.shape ==  (245760, 1)\n",
      " dataset_test_1.shape ==  (245760, 1)\n",
      " dataset_test_2.shape ==  (245760, 1)\n",
      "-----------------------------------------\n",
      " dataset_train_1.shape ==  (245760, 1)\n",
      " dataset_train_2.shape ==  (245760, 1)\n",
      " dataset_test_1.shape ==  (245760, 1)\n",
      " dataset_test_2.shape ==  (245760, 1)\n",
      "-----------------------------------------\n",
      "mean of dataset_train_1 ==  0.45230213\n",
      "variance of dataset_train_1 ==  0.0010688052\n",
      "standard deviation of dataset_train_1 ==  0.032692585\n",
      "-----------------------------------------\n",
      "mean of dataset_train_1 ==  0.5091193\n",
      "variance of dataset_train_2 ==  0.0024536815\n",
      "standard deviation of dataset_train_2 ==  0.04953465\n",
      "-----------------------------------------\n",
      "mean of dataset_test_1 ==  0.5687847\n",
      "variance of dataset_test_1 ==  0.0011410454\n",
      "standard deviation of dataset_test_1 ==  0.033779364\n",
      "-----------------------------------------\n",
      "mean of dataset_test_2 ==  0.46432123\n",
      "variance of dataset_test_2 ==  0.0029907485\n",
      "standard deviation of dataset_test_2 ==  0.054687735\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "length of train_1  ==  245760\n",
      "length of train_2 ==  245760\n",
      "length of test_1 ==  245760\n",
      "length of test_2 ==  245760\n",
      " Lets start ~!!! \n",
      "-----------------------------------------\n",
      "-----------------------------------\n",
      " train_X.shape =  (245730, 29, 1)\n",
      " train_Y.shape =  (245730,)\n",
      " train_X.shape =  (245730, 29, 1)\n",
      " train_Y.shape =  (245730,)\n",
      " test_X.shape ==  (245730, 29, 1)\n",
      " test_Y.shape ==  (245730,)\n",
      " test_X.shape ==  (245730, 29, 1)\n",
      " test_Y.shape ==  (245730,)\n",
      "-----------------------------------\n",
      "Train on 245730 samples, validate on 245730 samples\n",
      "Epoch 1/10\n",
      "245730/245730 [==============================] - 20s 82us/step - loss: 0.0878 - first_output_model1_loss: 0.0107 - second_output_model1_loss: 0.0160 - first_output_model1_mean_squared_error: 0.0107 - first_output_model1_mean_absolute_error: 0.0609 - first_output_model1_mean_absolute_percentage_error: 1844.4926 - second_output_model1_mean_squared_error: 0.0160 - second_output_model1_mean_absolute_error: 0.0779 - second_output_model1_mean_absolute_percentage_error: 2306.1794 - val_loss: 0.0751 - val_first_output_model1_loss: 0.0110 - val_second_output_model1_loss: 0.0080 - val_first_output_model1_mean_squared_error: 0.0110 - val_first_output_model1_mean_absolute_error: 0.0996 - val_first_output_model1_mean_absolute_percentage_error: 1910.1063 - val_second_output_model1_mean_squared_error: 0.0080 - val_second_output_model1_mean_absolute_error: 0.0764 - val_second_output_model1_mean_absolute_percentage_error: 2171.2661\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.08784, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.01-_0.09.hdf5\n",
      "Epoch 2/10\n",
      "245730/245730 [==============================] - 16s 65us/step - loss: 0.0561 - first_output_model1_loss: 0.0017 - second_output_model1_loss: 0.0032 - first_output_model1_mean_squared_error: 0.0017 - first_output_model1_mean_absolute_error: 0.0318 - first_output_model1_mean_absolute_percentage_error: 1454.2502 - second_output_model1_mean_squared_error: 0.0032 - second_output_model1_mean_absolute_error: 0.0432 - second_output_model1_mean_absolute_percentage_error: 2046.4453 - val_loss: 0.0646 - val_first_output_model1_loss: 0.0112 - val_second_output_model1_loss: 0.0069 - val_first_output_model1_mean_squared_error: 0.0112 - val_first_output_model1_mean_absolute_error: 0.1005 - val_first_output_model1_mean_absolute_percentage_error: 1908.0670 - val_second_output_model1_mean_squared_error: 0.0069 - val_second_output_model1_mean_absolute_error: 0.0696 - val_second_output_model1_mean_absolute_percentage_error: 2135.7632\n",
      "\n",
      "Epoch 00002: loss improved from 0.08784 to 0.05610, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.02-_0.06.hdf5\n",
      "Epoch 3/10\n",
      "245730/245730 [==============================] - 16s 64us/step - loss: 0.0466 - first_output_model1_loss: 0.0015 - second_output_model1_loss: 0.0029 - first_output_model1_mean_squared_error: 0.0015 - first_output_model1_mean_absolute_error: 0.0298 - first_output_model1_mean_absolute_percentage_error: 1766.0283 - second_output_model1_mean_squared_error: 0.0029 - second_output_model1_mean_absolute_error: 0.0412 - second_output_model1_mean_absolute_percentage_error: 2133.0438 - val_loss: 0.0565 - val_first_output_model1_loss: 0.0119 - val_second_output_model1_loss: 0.0065 - val_first_output_model1_mean_squared_error: 0.0119 - val_first_output_model1_mean_absolute_error: 0.1041 - val_first_output_model1_mean_absolute_percentage_error: 1894.5768 - val_second_output_model1_mean_squared_error: 0.0065 - val_second_output_model1_mean_absolute_error: 0.0675 - val_second_output_model1_mean_absolute_percentage_error: 2129.6938\n",
      "\n",
      "Epoch 00003: loss improved from 0.05610 to 0.04662, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.03-_0.05.hdf5\n",
      "Epoch 4/10\n",
      "245730/245730 [==============================] - 16s 65us/step - loss: 0.0386 - first_output_model1_loss: 0.0014 - second_output_model1_loss: 0.0028 - first_output_model1_mean_squared_error: 0.0014 - first_output_model1_mean_absolute_error: 0.0286 - first_output_model1_mean_absolute_percentage_error: 1938.3495 - second_output_model1_mean_squared_error: 0.0028 - second_output_model1_mean_absolute_error: 0.0400 - second_output_model1_mean_absolute_percentage_error: 2048.6902 - val_loss: 0.0494 - val_first_output_model1_loss: 0.0122 - val_second_output_model1_loss: 0.0063 - val_first_output_model1_mean_squared_error: 0.0122 - val_first_output_model1_mean_absolute_error: 0.1053 - val_first_output_model1_mean_absolute_percentage_error: 1889.7663 - val_second_output_model1_mean_squared_error: 0.0063 - val_second_output_model1_mean_absolute_error: 0.0660 - val_second_output_model1_mean_absolute_percentage_error: 2129.6978\n",
      "\n",
      "Epoch 00004: loss improved from 0.04662 to 0.03856, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.04-_0.04.hdf5\n",
      "Epoch 5/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0318 - first_output_model1_loss: 0.0013 - second_output_model1_loss: 0.0027 - first_output_model1_mean_squared_error: 0.0013 - first_output_model1_mean_absolute_error: 0.0277 - first_output_model1_mean_absolute_percentage_error: 1789.3184 - second_output_model1_mean_squared_error: 0.0027 - second_output_model1_mean_absolute_error: 0.0392 - second_output_model1_mean_absolute_percentage_error: 2147.2781 - val_loss: 0.0437 - val_first_output_model1_loss: 0.0126 - val_second_output_model1_loss: 0.0061 - val_first_output_model1_mean_squared_error: 0.0126 - val_first_output_model1_mean_absolute_error: 0.1075 - val_first_output_model1_mean_absolute_percentage_error: 1882.6937 - val_second_output_model1_mean_squared_error: 0.0061 - val_second_output_model1_mean_absolute_error: 0.0649 - val_second_output_model1_mean_absolute_percentage_error: 2128.4820\n",
      "\n",
      "Epoch 00005: loss improved from 0.03856 to 0.03181, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.05-_0.03.hdf5\n",
      "Epoch 6/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0262 - first_output_model1_loss: 0.0013 - second_output_model1_loss: 0.0026 - first_output_model1_mean_squared_error: 0.0013 - first_output_model1_mean_absolute_error: 0.0268 - first_output_model1_mean_absolute_percentage_error: 1837.0906 - second_output_model1_mean_squared_error: 0.0026 - second_output_model1_mean_absolute_error: 0.0384 - second_output_model1_mean_absolute_percentage_error: 2122.6318 - val_loss: 0.0389 - val_first_output_model1_loss: 0.0131 - val_second_output_model1_loss: 0.0058 - val_first_output_model1_mean_squared_error: 0.0131 - val_first_output_model1_mean_absolute_error: 0.1095 - val_first_output_model1_mean_absolute_percentage_error: 1876.2537 - val_second_output_model1_mean_squared_error: 0.0058 - val_second_output_model1_mean_absolute_error: 0.0630 - val_second_output_model1_mean_absolute_percentage_error: 2121.5170\n",
      "\n",
      "Epoch 00006: loss improved from 0.03181 to 0.02622, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.06-_0.03.hdf5\n",
      "Epoch 7/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0217 - first_output_model1_loss: 0.0012 - second_output_model1_loss: 0.0025 - first_output_model1_mean_squared_error: 0.0012 - first_output_model1_mean_absolute_error: 0.0262 - first_output_model1_mean_absolute_percentage_error: 1837.2975 - second_output_model1_mean_squared_error: 0.0025 - second_output_model1_mean_absolute_error: 0.0379 - second_output_model1_mean_absolute_percentage_error: 1957.9857 - val_loss: 0.0347 - val_first_output_model1_loss: 0.0128 - val_second_output_model1_loss: 0.0059 - val_first_output_model1_mean_squared_error: 0.0128 - val_first_output_model1_mean_absolute_error: 0.1082 - val_first_output_model1_mean_absolute_percentage_error: 1882.9309 - val_second_output_model1_mean_squared_error: 0.0059 - val_second_output_model1_mean_absolute_error: 0.0632 - val_second_output_model1_mean_absolute_percentage_error: 2125.6898\n",
      "\n",
      "Epoch 00007: loss improved from 0.02622 to 0.02166, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.07-_0.02.hdf5\n",
      "Epoch 8/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0180 - first_output_model1_loss: 0.0012 - second_output_model1_loss: 0.0025 - first_output_model1_mean_squared_error: 0.0012 - first_output_model1_mean_absolute_error: 0.0258 - first_output_model1_mean_absolute_percentage_error: 1836.6261 - second_output_model1_mean_squared_error: 0.0025 - second_output_model1_mean_absolute_error: 0.0376 - second_output_model1_mean_absolute_percentage_error: 2035.1199 - val_loss: 0.0317 - val_first_output_model1_loss: 0.0132 - val_second_output_model1_loss: 0.0057 - val_first_output_model1_mean_squared_error: 0.0132 - val_first_output_model1_mean_absolute_error: 0.1100 - val_first_output_model1_mean_absolute_percentage_error: 1876.1254 - val_second_output_model1_mean_squared_error: 0.0057 - val_second_output_model1_mean_absolute_error: 0.0622 - val_second_output_model1_mean_absolute_percentage_error: 2122.8822\n",
      "\n",
      "Epoch 00008: loss improved from 0.02166 to 0.01797, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.08-_0.02.hdf5\n",
      "Epoch 9/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0150 - first_output_model1_loss: 0.0011 - second_output_model1_loss: 0.0025 - first_output_model1_mean_squared_error: 0.0011 - first_output_model1_mean_absolute_error: 0.0255 - first_output_model1_mean_absolute_percentage_error: 1813.7345 - second_output_model1_mean_squared_error: 0.0025 - second_output_model1_mean_absolute_error: 0.0374 - second_output_model1_mean_absolute_percentage_error: 2146.1804 - val_loss: 0.0291 - val_first_output_model1_loss: 0.0131 - val_second_output_model1_loss: 0.0059 - val_first_output_model1_mean_squared_error: 0.0131 - val_first_output_model1_mean_absolute_error: 0.1097 - val_first_output_model1_mean_absolute_percentage_error: 1877.3411 - val_second_output_model1_mean_squared_error: 0.0059 - val_second_output_model1_mean_absolute_error: 0.0631 - val_second_output_model1_mean_absolute_percentage_error: 2129.6982\n",
      "\n",
      "Epoch 00009: loss improved from 0.01797 to 0.01500, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.09-_0.02.hdf5\n",
      "Epoch 10/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0126 - first_output_model1_loss: 0.0011 - second_output_model1_loss: 0.0024 - first_output_model1_mean_squared_error: 0.0011 - first_output_model1_mean_absolute_error: 0.0252 - first_output_model1_mean_absolute_percentage_error: 1842.1399 - second_output_model1_mean_squared_error: 0.0024 - second_output_model1_mean_absolute_error: 0.0372 - second_output_model1_mean_absolute_percentage_error: 2056.7379 - val_loss: 0.0270 - val_first_output_model1_loss: 0.0131 - val_second_output_model1_loss: 0.0058 - val_first_output_model1_mean_squared_error: 0.0131 - val_first_output_model1_mean_absolute_error: 0.1098 - val_first_output_model1_mean_absolute_percentage_error: 1876.8599 - val_second_output_model1_mean_squared_error: 0.0058 - val_second_output_model1_mean_absolute_error: 0.0628 - val_second_output_model1_mean_absolute_percentage_error: 2129.5161\n",
      "\n",
      "Epoch 00010: loss improved from 0.01500 to 0.01263, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow.10-_0.01.hdf5\n",
      "> Saved trained model : 01_01_bearing3_shallow-CL_deep-CL_10e_model1_conv_shallow10.h5\n",
      "Train on 245730 samples, validate on 245730 samples\n",
      "Epoch 1/10\n",
      "245730/245730 [==============================] - 47s 189us/step - loss: 0.1888 - first_output_model2_loss: 0.0264 - second_output_model2_loss: 0.0495 - first_output_model2_mean_squared_error: 0.0264 - first_output_model2_mean_absolute_error: 0.1079 - first_output_model2_mean_absolute_percentage_error: 1824.6371 - second_output_model2_mean_squared_error: 0.0495 - second_output_model2_mean_absolute_error: 0.1458 - second_output_model2_mean_absolute_percentage_error: 2260.4770 - val_loss: 0.1172 - val_first_output_model2_loss: 0.0122 - val_second_output_model2_loss: 0.0061 - val_first_output_model2_mean_squared_error: 0.0122 - val_first_output_model2_mean_absolute_error: 0.1057 - val_first_output_model2_mean_absolute_percentage_error: 1900.9792 - val_second_output_model2_mean_squared_error: 0.0061 - val_second_output_model2_mean_absolute_error: 0.0647 - val_second_output_model2_mean_absolute_percentage_error: 2127.8582\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.18876, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.01-_0.19.hdf5\n",
      "Epoch 2/10\n",
      "245730/245730 [==============================] - 44s 178us/step - loss: 0.0920 - first_output_model2_loss: 0.0021 - second_output_model2_loss: 0.0037 - first_output_model2_mean_squared_error: 0.0021 - first_output_model2_mean_absolute_error: 0.0347 - first_output_model2_mean_absolute_percentage_error: 1822.2035 - second_output_model2_mean_squared_error: 0.0037 - second_output_model2_mean_absolute_error: 0.0464 - second_output_model2_mean_absolute_percentage_error: 2193.2128 - val_loss: 0.0936 - val_first_output_model2_loss: 0.0141 - val_second_output_model2_loss: 0.0050 - val_first_output_model2_mean_squared_error: 0.0141 - val_first_output_model2_mean_absolute_error: 0.1139 - val_first_output_model2_mean_absolute_percentage_error: 1872.3722 - val_second_output_model2_mean_squared_error: 0.0050 - val_second_output_model2_mean_absolute_error: 0.0571 - val_second_output_model2_mean_absolute_percentage_error: 2088.1692\n",
      "\n",
      "Epoch 00002: loss improved from 0.18876 to 0.09202, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.02-_0.09.hdf5\n",
      "Epoch 3/10\n",
      "245730/245730 [==============================] - 44s 180us/step - loss: 0.0682 - first_output_model2_loss: 0.0012 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0012 - first_output_model2_mean_absolute_error: 0.0257 - first_output_model2_mean_absolute_percentage_error: 1842.3861 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0381 - second_output_model2_mean_absolute_percentage_error: 2126.2513 - val_loss: 0.0751 - val_first_output_model2_loss: 0.0151 - val_second_output_model2_loss: 0.0047 - val_first_output_model2_mean_squared_error: 0.0151 - val_first_output_model2_mean_absolute_error: 0.1182 - val_first_output_model2_mean_absolute_percentage_error: 1855.7350 - val_second_output_model2_mean_squared_error: 0.0047 - val_second_output_model2_mean_absolute_error: 0.0545 - val_second_output_model2_mean_absolute_percentage_error: 2071.0177\n",
      "\n",
      "Epoch 00003: loss improved from 0.09202 to 0.06824, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.03-_0.07.hdf5\n",
      "Epoch 4/10\n",
      "245730/245730 [==============================] - 44s 179us/step - loss: 0.0514 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0250 - first_output_model2_mean_absolute_percentage_error: 1881.5267 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0377 - second_output_model2_mean_absolute_percentage_error: 2064.1549 - val_loss: 0.0610 - val_first_output_model2_loss: 0.0155 - val_second_output_model2_loss: 0.0046 - val_first_output_model2_mean_squared_error: 0.0155 - val_first_output_model2_mean_absolute_error: 0.1200 - val_first_output_model2_mean_absolute_percentage_error: 1848.4060 - val_second_output_model2_mean_squared_error: 0.0046 - val_second_output_model2_mean_absolute_error: 0.0540 - val_second_output_model2_mean_absolute_percentage_error: 2067.2887\n",
      "\n",
      "Epoch 00004: loss improved from 0.06824 to 0.05140, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.04-_0.05.hdf5\n",
      "Epoch 5/10\n",
      "245730/245730 [==============================] - 43s 177us/step - loss: 0.0388 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0248 - first_output_model2_mean_absolute_percentage_error: 1846.9475 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0376 - second_output_model2_mean_absolute_percentage_error: 2088.6751 - val_loss: 0.0504 - val_first_output_model2_loss: 0.0156 - val_second_output_model2_loss: 0.0048 - val_first_output_model2_mean_squared_error: 0.0156 - val_first_output_model2_mean_absolute_error: 0.1203 - val_first_output_model2_mean_absolute_percentage_error: 1847.3809 - val_second_output_model2_mean_squared_error: 0.0048 - val_second_output_model2_mean_absolute_error: 0.0551 - val_second_output_model2_mean_absolute_percentage_error: 2074.4331\n",
      "\n",
      "Epoch 00005: loss improved from 0.05140 to 0.03880, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.05-_0.04.hdf5\n",
      "Epoch 6/10\n",
      "245730/245730 [==============================] - 44s 178us/step - loss: 0.0295 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0247 - first_output_model2_mean_absolute_percentage_error: 1878.7381 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0375 - second_output_model2_mean_absolute_percentage_error: 2058.4766 - val_loss: 0.0423 - val_first_output_model2_loss: 0.0153 - val_second_output_model2_loss: 0.0050 - val_first_output_model2_mean_squared_error: 0.0153 - val_first_output_model2_mean_absolute_error: 0.1190 - val_first_output_model2_mean_absolute_percentage_error: 1852.2496 - val_second_output_model2_mean_squared_error: 0.0050 - val_second_output_model2_mean_absolute_error: 0.0564 - val_second_output_model2_mean_absolute_percentage_error: 2083.5713\n",
      "\n",
      "Epoch 00006: loss improved from 0.03880 to 0.02946, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.06-_0.03.hdf5\n",
      "Epoch 7/10\n",
      "245730/245730 [==============================] - 44s 179us/step - loss: 0.0226 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0247 - first_output_model2_mean_absolute_percentage_error: 1853.5328 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0375 - second_output_model2_mean_absolute_percentage_error: 2023.2181 - val_loss: 0.0361 - val_first_output_model2_loss: 0.0149 - val_second_output_model2_loss: 0.0049 - val_first_output_model2_mean_squared_error: 0.0149 - val_first_output_model2_mean_absolute_error: 0.1176 - val_first_output_model2_mean_absolute_percentage_error: 1857.9214 - val_second_output_model2_mean_squared_error: 0.0049 - val_second_output_model2_mean_absolute_error: 0.0561 - val_second_output_model2_mean_absolute_percentage_error: 2081.5883\n",
      "\n",
      "Epoch 00007: loss improved from 0.02946 to 0.02260, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.07-_0.02.hdf5\n",
      "Epoch 8/10\n",
      "245730/245730 [==============================] - 44s 181us/step - loss: 0.0176 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0247 - first_output_model2_mean_absolute_percentage_error: 1851.8065 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0374 - second_output_model2_mean_absolute_percentage_error: 2076.3202 - val_loss: 0.0314 - val_first_output_model2_loss: 0.0145 - val_second_output_model2_loss: 0.0049 - val_first_output_model2_mean_squared_error: 0.0145 - val_first_output_model2_mean_absolute_error: 0.1159 - val_first_output_model2_mean_absolute_percentage_error: 1864.5078 - val_second_output_model2_mean_squared_error: 0.0049 - val_second_output_model2_mean_absolute_error: 0.0560 - val_second_output_model2_mean_absolute_percentage_error: 2080.3317\n",
      "\n",
      "Epoch 00008: loss improved from 0.02260 to 0.01759, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.08-_0.02.hdf5\n",
      "Epoch 9/10\n",
      "245730/245730 [==============================] - 44s 179us/step - loss: 0.0139 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0246 - first_output_model2_mean_absolute_percentage_error: 1838.2864 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0374 - second_output_model2_mean_absolute_percentage_error: 2078.2006 - val_loss: 0.0281 - val_first_output_model2_loss: 0.0144 - val_second_output_model2_loss: 0.0048 - val_first_output_model2_mean_squared_error: 0.0144 - val_first_output_model2_mean_absolute_error: 0.1156 - val_first_output_model2_mean_absolute_percentage_error: 1865.7014 - val_second_output_model2_mean_squared_error: 0.0048 - val_second_output_model2_mean_absolute_error: 0.0552 - val_second_output_model2_mean_absolute_percentage_error: 2075.0357\n",
      "\n",
      "Epoch 00009: loss improved from 0.01759 to 0.01394, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.09-_0.01.hdf5\n",
      "Epoch 10/10\n",
      "245730/245730 [==============================] - 44s 180us/step - loss: 0.0113 - first_output_model2_loss: 0.0011 - second_output_model2_loss: 0.0025 - first_output_model2_mean_squared_error: 0.0011 - first_output_model2_mean_absolute_error: 0.0246 - first_output_model2_mean_absolute_percentage_error: 1840.8653 - second_output_model2_mean_squared_error: 0.0025 - second_output_model2_mean_absolute_error: 0.0374 - second_output_model2_mean_absolute_percentage_error: 2061.4705 - val_loss: 0.0258 - val_first_output_model2_loss: 0.0144 - val_second_output_model2_loss: 0.0048 - val_first_output_model2_mean_squared_error: 0.0144 - val_first_output_model2_mean_absolute_error: 0.1154 - val_first_output_model2_mean_absolute_percentage_error: 1866.3093 - val_second_output_model2_mean_squared_error: 0.0048 - val_second_output_model2_mean_absolute_error: 0.0552 - val_second_output_model2_mean_absolute_percentage_error: 2075.0767\n",
      "\n",
      "Epoch 00010: loss improved from 0.01394 to 0.01127, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow.10-_0.01.hdf5\n",
      "> Saved trained model : 01_01_bearing3_shallow-CL_deep-CL_10e_model2_lstm_shallow10.h5\n",
      "Train on 245730 samples, validate on 245730 samples\n",
      "Epoch 1/10\n",
      "245730/245730 [==============================] - 19s 76us/step - loss: 0.1063 - first_output_model3_loss: 0.0171 - second_output_model3_loss: 0.0280 - first_output_model3_mean_squared_error: 0.0171 - first_output_model3_mean_absolute_error: 0.0812 - first_output_model3_mean_absolute_percentage_error: 1942.6335 - second_output_model3_mean_squared_error: 0.0280 - second_output_model3_mean_absolute_error: 0.1040 - second_output_model3_mean_absolute_percentage_error: 2292.4203 - val_loss: 0.0768 - val_first_output_model3_loss: 0.0126 - val_second_output_model3_loss: 0.0065 - val_first_output_model3_mean_squared_error: 0.0126 - val_first_output_model3_mean_absolute_error: 0.1071 - val_first_output_model3_mean_absolute_percentage_error: 1881.3035 - val_second_output_model3_mean_squared_error: 0.0065 - val_second_output_model3_mean_absolute_error: 0.0674 - val_second_output_model3_mean_absolute_percentage_error: 2142.3941\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.10634, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.01-_0.11.hdf5\n",
      "Epoch 2/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0600 - first_output_model3_loss: 0.0021 - second_output_model3_loss: 0.0038 - first_output_model3_mean_squared_error: 0.0021 - first_output_model3_mean_absolute_error: 0.0356 - first_output_model3_mean_absolute_percentage_error: 1824.5223 - second_output_model3_mean_squared_error: 0.0038 - second_output_model3_mean_absolute_error: 0.0473 - second_output_model3_mean_absolute_percentage_error: 1999.8309 - val_loss: 0.0714 - val_first_output_model3_loss: 0.0158 - val_second_output_model3_loss: 0.0051 - val_first_output_model3_mean_squared_error: 0.0158 - val_first_output_model3_mean_absolute_error: 0.1211 - val_first_output_model3_mean_absolute_percentage_error: 1825.4144 - val_second_output_model3_mean_squared_error: 0.0051 - val_second_output_model3_mean_absolute_error: 0.0576 - val_second_output_model3_mean_absolute_percentage_error: 2089.0785\n",
      "\n",
      "Epoch 00002: loss improved from 0.10634 to 0.06001, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.02-_0.06.hdf5\n",
      "Epoch 3/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0518 - first_output_model3_loss: 0.0016 - second_output_model3_loss: 0.0031 - first_output_model3_mean_squared_error: 0.0016 - first_output_model3_mean_absolute_error: 0.0305 - first_output_model3_mean_absolute_percentage_error: 1717.5655 - second_output_model3_mean_squared_error: 0.0031 - second_output_model3_mean_absolute_error: 0.0429 - second_output_model3_mean_absolute_percentage_error: 2265.6498 - val_loss: 0.0644 - val_first_output_model3_loss: 0.0159 - val_second_output_model3_loss: 0.0049 - val_first_output_model3_mean_squared_error: 0.0159 - val_first_output_model3_mean_absolute_error: 0.1215 - val_first_output_model3_mean_absolute_percentage_error: 1823.7414 - val_second_output_model3_mean_squared_error: 0.0049 - val_second_output_model3_mean_absolute_error: 0.0561 - val_second_output_model3_mean_absolute_percentage_error: 2079.0008\n",
      "\n",
      "Epoch 00003: loss improved from 0.06001 to 0.05180, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.03-_0.05.hdf5\n",
      "Epoch 4/10\n",
      "245730/245730 [==============================] - 16s 65us/step - loss: 0.0449 - first_output_model3_loss: 0.0014 - second_output_model3_loss: 0.0029 - first_output_model3_mean_squared_error: 0.0014 - first_output_model3_mean_absolute_error: 0.0283 - first_output_model3_mean_absolute_percentage_error: 1765.7184 - second_output_model3_mean_squared_error: 0.0029 - second_output_model3_mean_absolute_error: 0.0411 - second_output_model3_mean_absolute_percentage_error: 2161.9402 - val_loss: 0.0582 - val_first_output_model3_loss: 0.0157 - val_second_output_model3_loss: 0.0049 - val_first_output_model3_mean_squared_error: 0.0157 - val_first_output_model3_mean_absolute_error: 0.1209 - val_first_output_model3_mean_absolute_percentage_error: 1828.0664 - val_second_output_model3_mean_squared_error: 0.0049 - val_second_output_model3_mean_absolute_error: 0.0566 - val_second_output_model3_mean_absolute_percentage_error: 2084.6589\n",
      "\n",
      "Epoch 00004: loss improved from 0.05180 to 0.04485, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.04-_0.04.hdf5\n",
      "Epoch 5/10\n",
      "245730/245730 [==============================] - 16s 65us/step - loss: 0.0388 - first_output_model3_loss: 0.0013 - second_output_model3_loss: 0.0027 - first_output_model3_mean_squared_error: 0.0013 - first_output_model3_mean_absolute_error: 0.0272 - first_output_model3_mean_absolute_percentage_error: 1719.7102 - second_output_model3_mean_squared_error: 0.0027 - second_output_model3_mean_absolute_error: 0.0398 - second_output_model3_mean_absolute_percentage_error: 2065.7622 - val_loss: 0.0527 - val_first_output_model3_loss: 0.0157 - val_second_output_model3_loss: 0.0049 - val_first_output_model3_mean_squared_error: 0.0157 - val_first_output_model3_mean_absolute_error: 0.1209 - val_first_output_model3_mean_absolute_percentage_error: 1830.2701 - val_second_output_model3_mean_squared_error: 0.0049 - val_second_output_model3_mean_absolute_error: 0.0565 - val_second_output_model3_mean_absolute_percentage_error: 2086.8598\n",
      "\n",
      "Epoch 00005: loss improved from 0.04485 to 0.03877, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.05-_0.04.hdf5\n",
      "Epoch 6/10\n",
      "245730/245730 [==============================] - 16s 67us/step - loss: 0.0334 - first_output_model3_loss: 0.0012 - second_output_model3_loss: 0.0026 - first_output_model3_mean_squared_error: 0.0012 - first_output_model3_mean_absolute_error: 0.0263 - first_output_model3_mean_absolute_percentage_error: 1827.5985 - second_output_model3_mean_squared_error: 0.0026 - second_output_model3_mean_absolute_error: 0.0390 - second_output_model3_mean_absolute_percentage_error: 2130.9813 - val_loss: 0.0474 - val_first_output_model3_loss: 0.0155 - val_second_output_model3_loss: 0.0047 - val_first_output_model3_mean_squared_error: 0.0155 - val_first_output_model3_mean_absolute_error: 0.1200 - val_first_output_model3_mean_absolute_percentage_error: 1836.4012 - val_second_output_model3_mean_squared_error: 0.0047 - val_second_output_model3_mean_absolute_error: 0.0549 - val_second_output_model3_mean_absolute_percentage_error: 2077.9117\n",
      "\n",
      "Epoch 00006: loss improved from 0.03877 to 0.03345, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.06-_0.03.hdf5\n",
      "Epoch 7/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0288 - first_output_model3_loss: 0.0012 - second_output_model3_loss: 0.0026 - first_output_model3_mean_squared_error: 0.0012 - first_output_model3_mean_absolute_error: 0.0259 - first_output_model3_mean_absolute_percentage_error: 1783.4576 - second_output_model3_mean_squared_error: 0.0026 - second_output_model3_mean_absolute_error: 0.0384 - second_output_model3_mean_absolute_percentage_error: 2099.3322 - val_loss: 0.0430 - val_first_output_model3_loss: 0.0151 - val_second_output_model3_loss: 0.0048 - val_first_output_model3_mean_squared_error: 0.0151 - val_first_output_model3_mean_absolute_error: 0.1185 - val_first_output_model3_mean_absolute_percentage_error: 1841.8174 - val_second_output_model3_mean_squared_error: 0.0048 - val_second_output_model3_mean_absolute_error: 0.0557 - val_second_output_model3_mean_absolute_percentage_error: 2084.6949\n",
      "\n",
      "Epoch 00007: loss improved from 0.03345 to 0.02883, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.07-_0.03.hdf5\n",
      "Epoch 8/10\n",
      "245730/245730 [==============================] - 16s 67us/step - loss: 0.0248 - first_output_model3_loss: 0.0011 - second_output_model3_loss: 0.0025 - first_output_model3_mean_squared_error: 0.0011 - first_output_model3_mean_absolute_error: 0.0255 - first_output_model3_mean_absolute_percentage_error: 1768.9235 - second_output_model3_mean_squared_error: 0.0025 - second_output_model3_mean_absolute_error: 0.0380 - second_output_model3_mean_absolute_percentage_error: 2118.6109 - val_loss: 0.0391 - val_first_output_model3_loss: 0.0151 - val_second_output_model3_loss: 0.0047 - val_first_output_model3_mean_squared_error: 0.0151 - val_first_output_model3_mean_absolute_error: 0.1182 - val_first_output_model3_mean_absolute_percentage_error: 1844.3804 - val_second_output_model3_mean_squared_error: 0.0047 - val_second_output_model3_mean_absolute_error: 0.0549 - val_second_output_model3_mean_absolute_percentage_error: 2076.1431\n",
      "\n",
      "Epoch 00008: loss improved from 0.02883 to 0.02482, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.08-_0.02.hdf5\n",
      "Epoch 9/10\n",
      "245730/245730 [==============================] - 16s 66us/step - loss: 0.0214 - first_output_model3_loss: 0.0011 - second_output_model3_loss: 0.0025 - first_output_model3_mean_squared_error: 0.0011 - first_output_model3_mean_absolute_error: 0.0253 - first_output_model3_mean_absolute_percentage_error: 1823.0186 - second_output_model3_mean_squared_error: 0.0025 - second_output_model3_mean_absolute_error: 0.0377 - second_output_model3_mean_absolute_percentage_error: 2065.5696 - val_loss: 0.0359 - val_first_output_model3_loss: 0.0150 - val_second_output_model3_loss: 0.0047 - val_first_output_model3_mean_squared_error: 0.0150 - val_first_output_model3_mean_absolute_error: 0.1180 - val_first_output_model3_mean_absolute_percentage_error: 1845.7563 - val_second_output_model3_mean_squared_error: 0.0047 - val_second_output_model3_mean_absolute_error: 0.0550 - val_second_output_model3_mean_absolute_percentage_error: 2076.4968\n",
      "\n",
      "Epoch 00009: loss improved from 0.02482 to 0.02137, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.09-_0.02.hdf5\n",
      "Epoch 10/10\n",
      "245730/245730 [==============================] - 16s 65us/step - loss: 0.0184 - first_output_model3_loss: 0.0011 - second_output_model3_loss: 0.0025 - first_output_model3_mean_squared_error: 0.0011 - first_output_model3_mean_absolute_error: 0.0251 - first_output_model3_mean_absolute_percentage_error: 1823.2536 - second_output_model3_mean_squared_error: 0.0025 - second_output_model3_mean_absolute_error: 0.0375 - second_output_model3_mean_absolute_percentage_error: 2117.9862 - val_loss: 0.0332 - val_first_output_model3_loss: 0.0149 - val_second_output_model3_loss: 0.0048 - val_first_output_model3_mean_squared_error: 0.0149 - val_first_output_model3_mean_absolute_error: 0.1177 - val_first_output_model3_mean_absolute_percentage_error: 1847.6301 - val_second_output_model3_mean_squared_error: 0.0048 - val_second_output_model3_mean_absolute_error: 0.0555 - val_second_output_model3_mean_absolute_percentage_error: 2080.0060\n",
      "\n",
      "Epoch 00010: loss improved from 0.02137 to 0.01841, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep.10-_0.02.hdf5\n",
      "> Saved trained model : 01_01_bearing3_shallow-CL_deep-CL_10e_model3_conv_deep10.h5\n",
      "Train on 245730 samples, validate on 245730 samples\n",
      "Epoch 1/10\n",
      "245730/245730 [==============================] - 141s 573us/step - loss: 0.2970 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0321 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9995 - second_output_model4_mean_squared_error: 0.0321 - second_output_model4_mean_absolute_error: 0.1155 - second_output_model4_mean_absolute_percentage_error: 2010.8405 - val_loss: 0.3847 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0062 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0062 - val_second_output_model4_mean_absolute_error: 0.0653 - val_second_output_model4_mean_absolute_percentage_error: 2132.1585\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.29697, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.01-_0.30.hdf5\n",
      "Epoch 2/10\n",
      "245730/245730 [==============================] - 133s 541us/step - loss: 0.2581 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0038 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0038 - second_output_model4_mean_absolute_error: 0.0474 - second_output_model4_mean_absolute_percentage_error: 1999.4105 - val_loss: 0.3733 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0050 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0050 - val_second_output_model4_mean_absolute_error: 0.0565 - val_second_output_model4_mean_absolute_percentage_error: 2081.7394\n",
      "\n",
      "Epoch 00002: loss improved from 0.29697 to 0.25806, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.02-_0.26.hdf5\n",
      "Epoch 3/10\n",
      "245730/245730 [==============================] - 133s 542us/step - loss: 0.2476 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0028 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0028 - second_output_model4_mean_absolute_error: 0.0400 - second_output_model4_mean_absolute_percentage_error: 2021.0261 - val_loss: 0.3640 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0044 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0044 - val_second_output_model4_mean_absolute_error: 0.0525 - val_second_output_model4_mean_absolute_percentage_error: 2055.9812\n",
      "\n",
      "Epoch 00003: loss improved from 0.25806 to 0.24760, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.03-_0.25.hdf5\n",
      "Epoch 4/10\n",
      "245730/245730 [==============================] - 133s 539us/step - loss: 0.2395 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0026 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0026 - second_output_model4_mean_absolute_error: 0.0383 - second_output_model4_mean_absolute_percentage_error: 2047.3598 - val_loss: 0.3569 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0045 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0045 - val_second_output_model4_mean_absolute_error: 0.0530 - val_second_output_model4_mean_absolute_percentage_error: 2059.9298\n",
      "\n",
      "Epoch 00004: loss improved from 0.24760 to 0.23946, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.04-_0.24.hdf5\n",
      "Epoch 5/10\n",
      "245730/245730 [==============================] - 134s 543us/step - loss: 0.2329 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0025 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0025 - second_output_model4_mean_absolute_error: 0.0379 - second_output_model4_mean_absolute_percentage_error: 2098.7357 - val_loss: 0.3513 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0047 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0047 - val_second_output_model4_mean_absolute_error: 0.0546 - val_second_output_model4_mean_absolute_percentage_error: 2071.3252\n",
      "\n",
      "Epoch 00005: loss improved from 0.23946 to 0.23293, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.05-_0.23.hdf5\n",
      "Epoch 6/10\n",
      "245730/245730 [==============================] - 134s 543us/step - loss: 0.2277 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0025 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0025 - second_output_model4_mean_absolute_error: 0.0377 - second_output_model4_mean_absolute_percentage_error: 2098.8351 - val_loss: 0.3469 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0049 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0049 - val_second_output_model4_mean_absolute_error: 0.0564 - val_second_output_model4_mean_absolute_percentage_error: 2083.4614\n",
      "\n",
      "Epoch 00006: loss improved from 0.23293 to 0.22766, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.06-_0.23.hdf5\n",
      "Epoch 7/10\n",
      "245730/245730 [==============================] - 133s 542us/step - loss: 0.2235 - first_output_model4_loss: 0.2056 - second_output_model4_loss: 0.0025 - first_output_model4_mean_squared_error: 0.2056 - first_output_model4_mean_absolute_error: 0.4523 - first_output_model4_mean_absolute_percentage_error: 99.9996 - second_output_model4_mean_squared_error: 0.0025 - second_output_model4_mean_absolute_error: 0.0376 - second_output_model4_mean_absolute_percentage_error: 2061.9556 - val_loss: 0.3432 - val_first_output_model4_loss: 0.3247 - val_second_output_model4_loss: 0.0050 - val_first_output_model4_mean_squared_error: 0.3247 - val_first_output_model4_mean_absolute_error: 0.5688 - val_first_output_model4_mean_absolute_percentage_error: 99.9996 - val_second_output_model4_mean_squared_error: 0.0050 - val_second_output_model4_mean_absolute_error: 0.0568 - val_second_output_model4_mean_absolute_percentage_error: 2085.9382\n",
      "\n",
      "Epoch 00007: loss improved from 0.22766 to 0.22346, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.07-_0.22.hdf5\n",
      "Epoch 8/10\n",
      "245730/245730 [==============================] - 134s 544us/step - loss: 0.2199 - first_output_model4_loss: 0.2055 - second_output_model4_loss: 0.0025 - first_output_model4_mean_squared_error: 0.2055 - first_output_model4_mean_absolute_error: 0.4521 - first_output_model4_mean_absolute_percentage_error: 99.9499 - second_output_model4_mean_squared_error: 0.0025 - second_output_model4_mean_absolute_error: 0.0375 - second_output_model4_mean_absolute_percentage_error: 2088.0562 - val_loss: 0.2285 - val_first_output_model4_loss: 0.2130 - val_second_output_model4_loss: 0.0049 - val_first_output_model4_mean_squared_error: 0.2130 - val_first_output_model4_mean_absolute_error: 0.4602 - val_first_output_model4_mean_absolute_percentage_error: 522.1087 - val_second_output_model4_mean_squared_error: 0.0049 - val_second_output_model4_mean_absolute_error: 0.0561 - val_second_output_model4_mean_absolute_percentage_error: 2081.6229\n",
      "\n",
      "Epoch 00008: loss improved from 0.22346 to 0.21994, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.08-_0.22.hdf5\n",
      "Epoch 9/10\n",
      "245730/245730 [==============================] - 133s 543us/step - loss: 0.0158 - first_output_model4_loss: 0.0034 - second_output_model4_loss: 0.0026 - first_output_model4_mean_squared_error: 0.0034 - first_output_model4_mean_absolute_error: 0.0378 - first_output_model4_mean_absolute_percentage_error: 1821.8968 - second_output_model4_mean_squared_error: 0.0026 - second_output_model4_mean_absolute_error: 0.0386 - second_output_model4_mean_absolute_percentage_error: 2119.5571 - val_loss: 0.0282 - val_first_output_model4_loss: 0.0140 - val_second_output_model4_loss: 0.0051 - val_first_output_model4_mean_squared_error: 0.0140 - val_first_output_model4_mean_absolute_error: 0.1136 - val_first_output_model4_mean_absolute_percentage_error: 1872.6908 - val_second_output_model4_mean_squared_error: 0.0051 - val_second_output_model4_mean_absolute_error: 0.0572 - val_second_output_model4_mean_absolute_percentage_error: 2088.0805\n",
      "\n",
      "Epoch 00009: loss improved from 0.21994 to 0.01580, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.09-_0.02.hdf5\n",
      "Epoch 10/10\n",
      "245730/245730 [==============================] - 134s 545us/step - loss: 0.0123 - first_output_model4_loss: 0.0012 - second_output_model4_loss: 0.0025 - first_output_model4_mean_squared_error: 0.0012 - first_output_model4_mean_absolute_error: 0.0264 - first_output_model4_mean_absolute_percentage_error: 1815.6895 - second_output_model4_mean_squared_error: 0.0025 - second_output_model4_mean_absolute_error: 0.0377 - second_output_model4_mean_absolute_percentage_error: 2075.6047 - val_loss: 0.0272 - val_first_output_model4_loss: 0.0142 - val_second_output_model4_loss: 0.0051 - val_first_output_model4_mean_squared_error: 0.0142 - val_first_output_model4_mean_absolute_error: 0.1144 - val_first_output_model4_mean_absolute_percentage_error: 1869.9344 - val_second_output_model4_mean_squared_error: 0.0051 - val_second_output_model4_mean_absolute_error: 0.0572 - val_second_output_model4_mean_absolute_percentage_error: 2088.2252\n",
      "\n",
      "Epoch 00010: loss improved from 0.01580 to 0.01227, saving model to 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep.10-_0.01.hdf5\n",
      "> Saved trained model : 01_01_bearing3_shallow-CL_deep-CL_10e_model4_lstm_deep10.h5\n",
      "--------------------------------\n",
      "out1... =  [<tf.Tensor 'first_output_model1/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'first_output_model2/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'first_output_model3/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'first_output_model4/Relu:0' shape=(?, 1) dtype=float32>]\n",
      "--------------------------------\n",
      "out2... =  [<tf.Tensor 'second_output_model1/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'second_output_model2/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'second_output_model3/Relu:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'second_output_model4/Relu:0' shape=(?, 1) dtype=float32>]\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "output1_var... =  Tensor(\"average_1/truediv:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "--------------------------------\n",
      "output2_var... =  Tensor(\"average_2/truediv:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "--------------------------------\n",
      " Start Prediction ... \n",
      " Train X_1 (bearing 3 x), Train X_2 (bearing 3 y) Predicting...\n",
      "245730/245730 [==============================] - 44s 180us/step\n",
      " Test X_1 (bearing 3 x), Test X_2 (bearing 3 y) Predicting...\n",
      "245730/245730 [==============================] - 42s 172us/step\n",
      " Prediction complete ...\n",
      "trainPredict_1 = model.predict([trainX_1, ], ) = [[0.45216578]\n",
      " [0.45158687]\n",
      " [0.45293906]\n",
      " ...\n",
      " [0.45211896]\n",
      " [0.45231396]\n",
      " [0.45201617]]\n",
      "trainPredict_1.shape =  (245730, 1)\n",
      "testPredict_1 = model.predict([testX_1, ], ) =  [[0.45311388]\n",
      " [0.45294043]\n",
      " [0.45454267]\n",
      " ...\n",
      " [0.45482168]\n",
      " [0.45543945]\n",
      " [0.45507234]]\n",
      "testPredict_1.shape =  (245730, 1)\n",
      "scaler.inverse_transform(numpy.array(trainPredic_1)) =  [[-0.11567356]\n",
      " [-0.11966341]\n",
      " [-0.11034411]\n",
      " ...\n",
      " [-0.11599624]\n",
      " [-0.11465233]\n",
      " [-0.11670466]]\n",
      "scaler.inverse_transform(numpy.array([trainY_1])) =  [[-0.47399995  0.05900005 -0.15399994 ... -0.04600003  0.11700002\n",
      "  -0.09500009]]\n",
      "scaler.inverse_transform(numpy.array(testPredic_1)) =  [[-0.9601906 ]\n",
      " [-0.9614583 ]\n",
      " [-0.9497476 ]\n",
      " ...\n",
      " [-0.94770837]\n",
      " [-0.943193  ]\n",
      " [-0.94587624]]\n",
      "scaler.inverse_transform(numpy.array([testY_1])) =  [[-0.5569999  -0.07800015 -0.14599994 ... -0.03199983 -0.01699995\n",
      "  -0.07300019]]\n",
      "scaler.inverse_transform(numpy.array(trainPredic_2)) =  [[-0.12112535]\n",
      " [-0.12320811]\n",
      " [-0.11376587]\n",
      " ...\n",
      " [-0.10667392]\n",
      " [-0.12109303]\n",
      " [-0.13540675]]\n",
      "scaler.inverse_transform(numpy.array(trainY_2)) =  [[ 0.01500001 -0.0440001   0.01199995 ... -0.18300004 -0.596\n",
      "  -0.14399998]]\n",
      "scaler.inverse_transform(numpy.array(testPredic_2)) =  [[0.08514448]\n",
      " [0.09475335]\n",
      " [0.11003231]\n",
      " ...\n",
      " [0.11706705]\n",
      " [0.11408443]\n",
      " [0.09977031]]\n",
      "scaler.inverse_transform(numpy.array(testY_1)) =  [[-0.02899999  0.06299996 -0.08100007 ...  0.05600001 -0.293\n",
      "  -0.327     ]]\n",
      "=========================================\n",
      "Train_1 Score: 0.22494 RMSE\n",
      "Test_1 Score: 0.86930 RMSE\n",
      "-----------------------------------------\n",
      "Train_2 Score: 0.23137 RMSE\n",
      "Test_2 Score: 0.33550 RMSE\n",
      "=========================================\n",
      "Train_1 Score: 0.05060 MSE\n",
      "Test_1 Score: 0.75569 MSE\n",
      "-----------------------------------------\n",
      "Train_2 Score: 0.05353 MSE\n",
      "Test_2 Score: 0.11256 MSE\n",
      "=========================================\n",
      "Train_1 Score: 0.16853 MAE\n",
      "Test_1 Score: 0.83542 MAE\n",
      "-----------------------------------------\n",
      "Train_2 Score: 0.17423 MAE\n",
      "Test_2 Score: 0.27085 MAE\n",
      "=========================================\n",
      "mean of trainPredict_1_mean ==  -0.11334172\n",
      "variance of trainPredict_1_var ==  3.6141846e-05\n",
      "standard deviation of trainPredict_1_std ==  0.0060118088\n",
      "-----------------------------------------\n",
      "mean of trainPredict_2_mean ==  -0.12007556\n",
      "variance of trainPredict_2_var ==  0.0001351277\n",
      "standard deviation of trainPredict_2_std ==  0.011624444\n",
      "-----------------------------------------\n",
      "mean of testPredict_1_mean ==  -0.94839287\n",
      "variance of testPredict_1_var ==  4.141243e-05\n",
      "standard deviation of testPredict_1_std ==  0.006435249\n",
      "-----------------------------------------\n",
      "mean of testPredict_2_mean ==  0.10367081\n",
      "variance of testPredict_2_var ==  0.00015746846\n",
      "standard deviation of testPredict_2_std ==  0.012548644\n",
      "=========================================\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "first_input (InputLayer)        (None, 29, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "second_input (InputLayer)       (None, 29, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 29, 8)        16          first_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 29, 8)        16          second_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 29, 16)       0           conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 29, 32)       4352        first_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 29, 32)       4352        second_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 29, 16)       272         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 29, 64)       0           lstm_7[0][0]                     \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 29, 128)      2176        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 29, 64)       33024       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 29, 16)       2064        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 29, 16)       5184        lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 29, 16)       0           conv1d_9[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 29, 64)       20736       lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 29, 16)       272         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 29, 64)       0           lstm_11[0][0]                    \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 29, 128)      2176        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 29, 64)       33024       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 29, 16)       2064        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 29, 16)       5184        lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 29, 128)      256         first_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 29, 128)      256         second_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 29, 16)       0           conv1d_12[0][0]                  \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 29, 64)       20736       lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 29, 256)      0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 29, 64)       16896       first_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 29, 64)       16896       second_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 29, 16)       272         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 29, 64)       0           lstm_14[0][0]                    \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 29, 128)      32896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 29, 128)      0           lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 29, 128)      2176        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 29, 64)       33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 29, 64)       8256        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 29, 32)       20608       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 29, 16)       2064        conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 29, 16)       5184        lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           33024       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 128)          82432       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 64)           20736       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 64)           20736       lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "first_output_model1 (Dense)     (None, 1)            65          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "first_output_model2 (Dense)     (None, 1)            129         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "first_output_model3 (Dense)     (None, 1)            65          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "first_output_model4 (Dense)     (None, 1)            65          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "second_output_model1 (Dense)    (None, 1)            65          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "second_output_model2 (Dense)    (None, 1)            129         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "second_output_model3 (Dense)    (None, 1)            65          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "second_output_model4 (Dense)    (None, 1)            65          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 1)            0           first_output_model1[0][0]        \n",
      "                                                                 first_output_model2[0][0]        \n",
      "                                                                 first_output_model3[0][0]        \n",
      "                                                                 first_output_model4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 1)            0           second_output_model1[0][0]       \n",
      "                                                                 second_output_model2[0][0]       \n",
      "                                                                 second_output_model3[0][0]       \n",
      "                                                                 second_output_model4[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 461,000\n",
      "Trainable params: 461,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "model summary ...  None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "history.___train___loss === [0.08784331935419416, 0.05610083577483534, 0.04661532263558984, 0.03855876495247358, 0.03181281182829318, 0.02622406374561087, 0.021655401329992042, 0.017965297170457292, 0.015000420324093537, 0.012630059749190611, 0.1887636332721014, 0.09201901301196443, 0.0682379988163706, 0.05139618060486282, 0.03879768536666983, 0.029460850026215485, 0.022602622403482354, 0.017590469846206554, 0.013935134552805161, 0.011266741567704698, 0.10633559019981673, 0.06001194027631763, 0.05179570678190832, 0.044852110409515415, 0.0387729450442976, 0.033447343117071926, 0.028825368372192024, 0.02481599922683406, 0.021366289208808724, 0.018409596100138478, 0.2969698807003976, 0.2580579722810082, 0.2476026034082767, 0.23946190310784402, 0.2329261132826256, 0.22765932659274157, 0.22345953975672522, 0.2199409309334843, 0.01579811362086446, 0.012268571953460861]\n",
      "history.___test___loss === [0.07513578889303066, 0.06457790065764003, 0.05654126471068083, 0.049394419893039644, 0.04371249950542626, 0.03894080743651461, 0.03466120123379916, 0.03165277612897346, 0.029115745900963264, 0.027015637637278645, 0.11719128531317996, 0.0936039932188289, 0.07510327174765372, 0.06099250778381226, 0.05039764516137751, 0.04231676799990108, 0.03609744643691923, 0.031418082469066376, 0.02813361602210496, 0.02583093287966212, 0.0767955597999603, 0.07135735578088831, 0.0644497333969291, 0.05819748300404547, 0.052704632403628805, 0.047412128068277194, 0.0429721595671475, 0.03911989689945413, 0.03593762116873191, 0.03321724834968691, 0.38468037272884176, 0.37326435054686996, 0.364028364703249, 0.3569109437851147, 0.3512947989306704, 0.3468542464786662, 0.34318457234693145, 0.2284612355064677, 0.028213880317720676, 0.02719775808821453]\n",
      "history.first_output_model1_mse = [0.010729200113809186, 0.0016958006134447207, 0.001512032821030122, 0.0014001952026822609, 0.0013178867333812066, 0.0012500855375634796, 0.0012017503807570044, 0.001167345875780548, 0.0011400942129652202, 0.0011220949958549475, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model1_mae = [0.0609318247248461, 0.031796148353464095, 0.02984839321610578, 0.028626900219166692, 0.027671238630618926, 0.02683447577715398, 0.02624499173663232, 0.02579603193139269, 0.02546404759829268, 0.02521829471974159, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model1_mape = [1844.492573846393, 1454.2501543573092, 1766.028252489591, 1938.349540795303, 1789.3183539158404, 1837.0906095501423, 1837.2975173986094, 1836.6261312819981, 1813.7345110653212, 1842.139946397883, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model1_mse = [0.01595801366188767, 0.003152849079642871, 0.0029015946212838533, 0.002752358456919315, 0.00265127441427264, 0.0025721624776410524, 0.0025112533568661314, 0.002474537922233311, 0.002451374303268384, 0.002432023152673078, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "histroy.second_output_model1_mae =  [0.077856811296674, 0.043188598625920116, 0.04122325605270346, 0.03997317636517074, 0.03915105946739505, 0.038445047142898786, 0.037916970677685645, 0.03759623498736621, 0.037388713899720157, 0.03721552864083953, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model1_mape =  [2306.17939403488, 2046.4453298386009, 2133.0437713272363, 2048.690218575737, 2147.2780776286645, 2122.6317596218523, 1957.9856771946943, 2035.1198932585144, 2146.1804496525215, 2056.7378864447296, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model1_mse =  [0.011028553069804267, 0.011199539778605926, 0.011930197386213363, 0.012178694351051312, 0.012643345960087343, 0.01308143783009915, 0.012784117629627023, 0.013173243793685936, 0.013113012544318675, 0.013131253255250502, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model1_mae =  [0.09961388854226005, 0.10050287527129487, 0.10407274344073632, 0.10528249413324442, 0.10748787791807903, 0.10952185626688135, 0.10818818352556499, 0.10996653713849576, 0.10970089100369486, 0.10978724252744428, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model1_mape = [1910.1062569676712, 1908.0669824669844, 1894.5767808628157, 1889.7662896455108, 1882.693727841162, 1876.253716566247, 1882.9308696900669, 1876.1253500327123, 1877.3411332450341, 1876.8599429652197, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model1_mse =  [0.00797941933392812, 0.006872067708129132, 0.0065354228406562095, 0.006302350367477522, 0.006133276765594328, 0.005849036193360185, 0.005879524711791452, 0.005726417646871574, 0.005852075434429232, 0.005811874090271265, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model1_mae =  [0.07638145317563604, 0.06961440758061284, 0.06748428375634966, 0.06599198480635118, 0.06489590527892661, 0.06299683569995265, 0.06321684060032268, 0.06219836278657737, 0.06307176596792108, 0.06280690822622227, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model1_mape =  [2171.2661322488166, 2135.763151701597, 2129.693842294049, 2129.6977751733543, 2128.4820027724227, 2121.5169781353165, 2125.6897780157324, 2122.882236305246, 2129.6981583166157, 2129.516124651106, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "--------------------------------------------------------\n",
      "history.first_output_model2_se =  [None, None, None, None, None, None, None, None, None, None, 0.026424659083679265, 0.002075129018691378, 0.0011528177377646011, 0.0011020945807183945, 0.0010908957105224709, 0.0010860613507527128, 0.0010825315015332907, 0.0010803562695536782, 0.0010789070052340693, 0.0010777862251157145, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model2_mae =  [None, None, None, None, None, None, None, None, None, None, 0.10785446572773372, 0.03471070333001829, 0.025663869580845884, 0.02496109078170756, 0.024804917411354998, 0.02474533144498475, 0.0246959531162866, 0.024663465087737996, 0.024642462525267365, 0.024628149189777498, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model2_mape =  [None, None, None, None, None, None, None, None, None, None, 1824.6371175624413, 1822.2035195054023, 1842.3861282628218, 1881.526661971142, 1846.9475430227046, 1878.73809335958, 1853.5327672688063, 1851.8064724923229, 1838.2864071365852, 1840.8653210626642, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model2_mse =  [None, None, None, None, None, None, None, None, None, None, 0.049503715693415044, 0.0036819081601322896, 0.002543384203144882, 0.002492298954595692, 0.0024804318064076884, 0.002473920753929264, 0.0024705221488233037, 0.0024680559311067553, 0.002467620314350893, 0.0024657781874170576, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model2_mae =  [None, None, None, None, None, None, None, None, None, None, 0.14582378841254082, 0.04638666129014491, 0.03812555791779456, 0.037655972352259974, 0.0375504643007575, 0.0374848755738948, 0.03745291236883088, 0.037433530928051836, 0.03742459085120152, 0.03741327968645832, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model2_mape =  [None, None, None, None, None, None, None, None, None, None, 2260.476977853407, 2193.212834803605, 2126.2512861744403, 2064.154929258834, 2088.6751498710696, 2058.4766330074517, 2023.2180825759838, 2076.320226794341, 2078.2005698282787, 2061.470472209932, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model2_mse = [None, None, None, None, None, None, None, None, None, None, 0.012241445940188164, 0.014069595411673042, 0.015054240900942347, 0.015498264552295785, 0.015561418096420336, 0.015259085969379915, 0.014913263907971782, 0.014515751140293041, 0.014442486009375458, 0.014405379502188918, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model2_mae =  [None, None, None, None, None, None, None, None, None, None, 0.10566717888232878, 0.11394788935160174, 0.11816662998062513, 0.12002175686191845, 0.12028455761480461, 0.11902759526153384, 0.11757406908426753, 0.11588101199366198, 0.11556652638325493, 0.1154068621806535, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model2_mape = [None, None, None, None, None, None, None, None, None, None, 1900.979214466517, 1872.3721743447263, 1855.7350413886472, 1848.4060391073033, 1847.3809268786397, 1852.2496291407763, 1857.9213830499157, 1864.5078071493488, 1865.7014240797364, 1866.3093313656102, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model2_mse = [None, None, None, None, None, None, None, None, None, None, 0.006138391601014151, 0.005046182983053366, 0.004688754522938676, 0.004620976460541905, 0.004766115882280341, 0.004951783444610237, 0.004911358304209884, 0.0048860295383522, 0.004780140602503727, 0.004781136805144564, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model2_mae =  [None, None, None, None, None, None, None, None, None, None, 0.06465958753039111, 0.05712583999737751, 0.05450651356951539, 0.054000534954938395, 0.05507936092970674, 0.05644213022019203, 0.05614779264472205, 0.05596257360837876, 0.05518342465370274, 0.05519079240420646, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model2_mape =  [None, None, None, None, None, None, None, None, None, None, 2127.858157139983, 2088.169239101558, 2071.017715901323, 2067.288673002659, 2074.433101869261, 2083.57129921566, 2081.5883134827636, 2080.331710079789, 2075.035657369244, 2075.0767371462034, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "--------------------------------------------------------\n",
      "history.first_output_model3_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.017143823484773543, 0.0021345593921129713, 0.0015875795754346585, 0.001375962512294606, 0.0012781827340889974, 0.0012111819997910878, 0.001174338814395116, 0.001144552595952599, 0.0011233597760015707, 0.0011082481227440545, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model3_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.08123146420375713, 0.03560693302468731, 0.030497636099313805, 0.028314822313816893, 0.027175940926961156, 0.026348735813013714, 0.025912737313557793, 0.02552953460474923, 0.02526915919898131, 0.02507327329178656, None, None, None, None, None, None, None, None, None, None]\n",
      "history.first_output_model3_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1942.6334692123517, 1824.5223342484899, 1717.5654515923586, 1765.7183922882361, 1719.7102446113272, 1827.5984819465284, 1783.4576480718815, 1768.9234894085012, 1823.0186319809152, 1823.2536292978905, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model3_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.028028223788540745, 0.0037802019886947957, 0.003139794143567925, 0.0028992221507466703, 0.002742592153309092, 0.0026383759837102024, 0.002570475671357816, 0.002521825009010284, 0.0024912762749274325, 0.0024690315823979138, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model3_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.10404849440536744, 0.04732485575051829, 0.042894621152476706, 0.04106949136872114, 0.039813483577845164, 0.03898746745377938, 0.03840608967714722, 0.03800413032967675, 0.0377494958537075, 0.0375381626499537, None, None, None, None, None, None, None, None, None, None]\n",
      "history.second_output_model3_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2292.420260809323, 1999.830925591845, 2265.6497960568036, 2161.9401893915647, 2065.7622391260525, 2130.981255364615, 2099.332249963672, 2118.610935561743, 2065.5695522623873, 2117.986232660668, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model3_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.012561370112476104, 0.01576515740058118, 0.0158699305616727, 0.015716881185790652, 0.015716347949665695, 0.015473241938694754, 0.015133833337269115, 0.01505533480556722, 0.015012800705185236, 0.014921595787906384, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model3_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.10706327099510589, 0.12107637857678162, 0.12153738440289844, 0.12093256605857361, 0.12094586783622796, 0.1199507860542411, 0.11853539851782441, 0.11821364069298618, 0.11804015032817597, 0.11766137709337893, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_first_output_model3_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1881.3035132499772, 1825.4144078525865, 1823.7414379646755, 1828.066413457547, 1830.2700608449506, 1836.401170548495, 1841.8174285233717, 1844.3804193518204, 1845.75627321563, 1847.6301091680323, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model3_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.006536532244083567, 0.005089872003442858, 0.0048774986391154004, 0.004941507027073752, 0.004934061296994904, 0.004711458249180491, 0.004824365669370173, 0.0047054615895122595, 0.00471361056132784, 0.004782532729622794, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model3_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.06741257753255472, 0.057564254338465605, 0.05605787146085033, 0.056551756920691004, 0.056514511258683495, 0.0548829679072318, 0.05573489227285292, 0.05488150024118259, 0.05495671978975999, 0.05549214125593416, None, None, None, None, None, None, None, None, None, None]\n",
      "history.val_second_output_model3_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2142.394118647497, 2089.078507864009, 2079.0007701637273, 2084.658917093927, 2086.8597965334166, 2077.9116835320915, 2084.6949417965902, 2076.1430866473306, 2076.496844168644, 2080.006021004397, None, None, None, None, None, None, None, None, None, None]\n",
      "--------------------------------------------------------\n",
      "history.first_output_model4_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.2056454619554132, 0.2056458229801135, 0.2056458229801135, 0.2056458229801135, 0.2056458229801135, 0.2056458229801135, 0.2056458229801135, 0.20545525379545332, 0.0033564843370619394, 0.0012136558249544798]\n",
      "history.first_output_model4_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.4523016067764113, 0.45230201201328685, 0.45230201201328685, 0.45230201201328685, 0.45230201201328685, 0.45230201201328685, 0.45230201201328685, 0.4520780443873534, 0.03775498627244647, 0.026391325366296684]\n",
      "history.first_output_model4_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 99.9995024709539, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.94986274286741, 1821.8968012188097, 1815.6895014209529]\n",
      "history.second_output_model4_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.032114450952926324, 0.003779432960035766, 0.002766931016146391, 0.002561006743608856, 0.0025199806986967357, 0.002499126809453339, 0.002490674135893073, 0.0024820374997211185, 0.0025862510984994164, 0.0025015586481491755]\n",
      "history.second_output_model4_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.11554221537696259, 0.04743175464822701, 0.040016792821110626, 0.038252125676959806, 0.03791060167510048, 0.037722628924119254, 0.03763289602478457, 0.03754868066347752, 0.03855019055680759, 0.03774068113241334]\n",
      "history.second_output_model4_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2010.8404945638217, 1999.4104959429778, 2021.0261032270676, 2047.35975544458, 2098.7356855524563, 2098.8350744740633, 2061.9555836782065, 2088.0562171152465, 2119.5571346840034, 2075.6046899530925]\n",
      "history.val_first_output_model4_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.32465844295189134, 0.32465844295189134, 0.32465844295189134, 0.32465844295189134, 0.32465844295189134, 0.32465844295189134, 0.32465844295189134, 0.2129571377995764, 0.013997013203677534, 0.014161996338321667]\n",
      "history.val_first_output_model4_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.5687858947781772, 0.5687858947781772, 0.5687858947781772, 0.5687858947781772, 0.5687858947781772, 0.5687858947781772, 0.5687858947781772, 0.4602356342306282, 0.11363522601719232, 0.11435435915695016]\n",
      "history.val_first_output_model4_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 99.99959304928173, 522.1086790717983, 1872.690753770351, 1869.9343697375093]\n",
      "history.val_second_output_model4_mse =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.006227396819162176, 0.004959035596348998, 0.0044171925195033565, 0.0044844556553048625, 0.004701937163454154, 0.004946487540893018, 0.005001060615311457, 0.004911324625147843, 0.005056609857027124, 0.005059221666996042]\n",
      "history.val_second_output_model4_mae =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.06526780720486802, 0.056491277724139415, 0.05246385008112461, 0.05297351429347386, 0.0546024389468876, 0.056401409922087385, 0.056798239410043624, 0.05614583285307363, 0.05720172920718917, 0.057220492207663926]\n",
      "history.val_second_output_model4_mape =  [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2132.1584772557308, 2081.739380752176, 2055.9811663940395, 2059.929834892083, 2071.32516863927, 2083.461354983128, 2085.938173597118, 2081.6228602946144, 2088.0804862671494, 2088.225157684539]\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Train_1_x_dataframe.describe() -------\n",
      "                   4\n",
      "count  245760.000000\n",
      "mean       -0.114734\n",
      "std         0.225318\n",
      "min        -3.232000\n",
      "25%        -0.247000\n",
      "50%        -0.112000\n",
      "75%         0.020000\n",
      "max         3.660000\n",
      "------- df_trainPredictPlot_1_x.describe() -------\n",
      "                   0\n",
      "count  245730.000000\n",
      "mean       -0.113342\n",
      "std         0.006012\n",
      "min        -0.188977\n",
      "25%        -0.116848\n",
      "50%        -0.113177\n",
      "75%        -0.109616\n",
      "max        -0.049243\n",
      "------- Train_2_y_dataframe.describe() -------\n",
      "                   4\n",
      "count  245760.000000\n",
      "mean       -0.114734\n",
      "std         0.225318\n",
      "min        -3.232000\n",
      "25%        -0.247000\n",
      "50%        -0.112000\n",
      "75%         0.020000\n",
      "max         3.660000\n",
      "------- df_trainPredictPlot_2_y.describe() -------\n",
      "                   0\n",
      "count  245730.000000\n",
      "mean       -0.120076\n",
      "std         0.011624\n",
      "min        -0.285542\n",
      "25%        -0.126959\n",
      "50%        -0.119963\n",
      "75%        -0.113054\n",
      "max         0.030164\n",
      "------- Test 1 x.describe() -------\n",
      "                   4\n",
      "count  245760.000000\n",
      "mean       -0.114752\n",
      "std         0.246894\n",
      "min        -4.272000\n",
      "25%        -0.256000\n",
      "50%        -0.112000\n",
      "75%         0.029000\n",
      "max         3.037000\n",
      "------- df_testPredictPlot_1_x.describe() -------\n",
      "                   0\n",
      "count  245730.000000\n",
      "mean       -0.948393\n",
      "std         0.006435\n",
      "min        -1.045039\n",
      "25%        -0.952015\n",
      "50%        -0.948175\n",
      "75%        -0.944441\n",
      "max        -0.900850\n",
      "------- Test 2 y.describe() -------\n",
      "                   5\n",
      "count  245760.000000\n",
      "mean       -0.114440\n",
      "std         0.257416\n",
      "min        -2.300000\n",
      "25%        -0.266000\n",
      "50%        -0.115000\n",
      "75%         0.034000\n",
      "max         2.407000\n",
      "------- df_testPredictPlot_2_y.describe() -------\n",
      "                   0\n",
      "count  245730.000000\n",
      "mean        0.103671\n",
      "std         0.012549\n",
      "min        -0.032547\n",
      "25%         0.096301\n",
      "50%         0.103750\n",
      "75%         0.111157\n",
      "max         0.236299\n",
      "--- end of codes\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "\"\"\" \n",
    "2019.2.7.\n",
    "\n",
    "ensemble deep learning for time series prediction (reviewing on IEEE JNL)\n",
    "\n",
    "by kwangsuk.ks.lee at gmail\n",
    "\n",
    "I proposes a novel ensemble deep learning architecture for time-series forecasting \n",
    "of bearing datasets (NASA Prognostics Data Repository) \n",
    "by combining two shallow models and two deep models, \n",
    "where each model is based on one-dimensional Convolutional Neural Network (1D CNN) \n",
    "and Long Short-Term Memory (LSTM). \n",
    "\n",
    "The shallow model consists of a few numbers of layers with a number of units, \n",
    "which reduces the computational complexity, so that it enables fast learning of the entire ensemble model. \n",
    "\n",
    "The deep model with a large number of layers and a small number of units can be \n",
    "adequately used for better generalization, and this model is combined with \n",
    "a residual network to further reduce overfitting and loss of the entire ensemble model. \n",
    "\n",
    "Model 1 of Conv1d Shallow : 256 - 128 - 64  (units)\n",
    "Model 2 of LSTM  Shallow : 128 - 32 - 128 (units)\n",
    "Model 3 of Conv1d Deep : 16 - 128 - 16  (1/2 units of shallow model)\n",
    "Model 4 of lstm Deep : 64 - 16  -  64  (1/2 units of shallow model) \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas     \n",
    "from pandas import read_csv, DataFrame, Series  \n",
    "import math\n",
    "import keras      \n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate \n",
    "from keras.layers import Activation, BatchNormalization, regularizers, Dropout\n",
    "\n",
    "from keras.layers import Average, normalization, TimeDistributed \n",
    "\n",
    "#from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import LSTM, Bidirectional \n",
    "from keras.utils import plot_model  # New \n",
    "from os import makedirs \n",
    "from sklearn import preprocessing  # New \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix  # New \n",
    "# http://scikit-learn.org/stable/modules/classes.html \n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "# seed = 2458\n",
    "numpy.random.seed() \n",
    "\n",
    "\n",
    "# setting of global variables \n",
    "title = '01_01_bearing3_shallow-CL_deep-CL_10e_' # head of file name \n",
    "epochs = 10  # Do not modification, plt.plot(... str(str(format(history.first_output_model1_mse[-40,-39-38,-37,-36,-35,-34,-33,-32,-31], ...      \n",
    "batch_size = 1024 # 128 or 256 or 512 \n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):   #history = LossHistory()\n",
    "    def init(self):       # history.init() \n",
    "        self.loss = []  # [] \n",
    "        self.val_loss = []  # []    \n",
    "        # model 1   history.history['first_output_model1_loss']\n",
    "        # self.first_output_model1_loss  = []   # [[], , , ]  # [[],[],[],[]] # [][][][]  # [ , , , ]  # list(, , , ) \n",
    "        # self.second_output_model1_loss = []   # [[], , , ]\n",
    "        self.first_output_model1_mse   = [] \n",
    "        self.first_output_model1_mae   = [] \n",
    "        self.first_output_model1_mape  = [] \n",
    "        self.second_output_model1_mse  = []\n",
    "        self.second_output_model1_mae  = []\n",
    "        self.second_output_model1_mape = []\n",
    "        # self.val_first_output_model1_loss  = [] \n",
    "        # self.val_second_output_model1_loss = [] \n",
    "        self.val_first_output_model1_mse   = []\n",
    "        self.val_first_output_model1_mae   = []\n",
    "        self.val_first_output_model1_mape  = []\n",
    "        self.val_second_output_model1_mse  = []\n",
    "        self.val_second_output_model1_mae  = [] # [[], , , ] \n",
    "        self.val_second_output_model1_mape = [] # [[], , , ] \n",
    "        \n",
    "        # model 2 \n",
    "        # self.first_output_model2_loss   = []  # [ ,[], , ]  # [[],[],[],[]] # [][][][]  # [ , , , ]  # list(, , , )   \n",
    "        # self.second_output_model2_loss  = []  # [ ,[], , ]   \n",
    "        self.first_output_model2_mse    = []\n",
    "        self.first_output_model2_mae    = []\n",
    "        self.first_output_model2_mape   = []\n",
    "        self.second_output_model2_mse   = []\n",
    "        self.second_output_model2_mae   = []\n",
    "        self.second_output_model2_mape  = []\n",
    "        # self.val_first_output_model2_loss  = [] \n",
    "        # self.val_second_output_model2_loss = [] \n",
    "        self.val_first_output_model2_mse   = []\n",
    "        self.val_first_output_model2_mae   = []  \n",
    "        self.val_first_output_model2_mape  = []  \n",
    "        self.val_second_output_model2_mse  = []  \n",
    "        self.val_second_output_model2_mae  = [] # [ ,[], , ]\n",
    "        self.val_second_output_model2_mape = [] # [ ,[], , ]\n",
    "        \n",
    "        # model 3 \n",
    "        #self.first_output_model3_loss  = [] # [ , , [], ]  # [[],[],[],[]] # [][][][]  # [ , , , ]  # list(, , , ) \n",
    "        #self.second_output_model3_loss = [] # [ , , [], ] \n",
    "        self.first_output_model3_mse   = []\n",
    "        self.first_output_model3_mae   = []\n",
    "        self.first_output_model3_mape  = []\n",
    "        self.second_output_model3_mse  = []\n",
    "        self.second_output_model3_mae  = []\n",
    "        self.second_output_model3_mape = []\n",
    "        #self.val_first_output_model3_loss = []  \n",
    "        #self.val_second_output_model3_loss = []\n",
    "        self.val_first_output_model3_mse   = []\n",
    "        self.val_first_output_model3_mae   = []\n",
    "        self.val_first_output_model3_mape  = []\n",
    "        self.val_second_output_model3_mse  = []\n",
    "        self.val_second_output_model3_mae  = [] # [ , , [], ] \n",
    "        self.val_second_output_model3_mape = [] # [ , , [], ] \n",
    "        \n",
    "        # model 4 \n",
    "        #self.first_output_model4_loss = []  #  [ , , , []]  # [[],[],[],[]] # [][][][]  # [ , , , ]  # list(, , , ) \n",
    "        #self.second_output_model4_loss = [] #  [ , , , []] \n",
    "        self.first_output_model4_mse = [] \n",
    "        self.first_output_model4_mae = []\n",
    "        self.first_output_model4_mape = []\n",
    "        self.second_output_model4_mse = []\n",
    "        self.second_output_model4_mae = []\n",
    "        self.second_output_model4_mape = []\n",
    "                                          \n",
    "        #self.val_first_output_model4_loss = [] \n",
    "        #self.val_second_output_model4_loss = []                 \n",
    "        self.val_first_output_model4_mse = []\n",
    "        self.val_first_output_model4_mae = []\n",
    "        self.val_first_output_model4_mape = []\n",
    "        self.val_second_output_model4_mse = []\n",
    "        self.val_second_output_model4_mae = []  # [ , , ,[]]\n",
    "        self.val_second_output_model4_mape = [] # [ , , ,[]] \n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):  #def on_epoch_end(self, batch, logs={}):\n",
    "        # self.loss.append(logs.get('loss'))  \n",
    "        # self.val_loss.append(logs.get('val_loss'))  \n",
    "        # self.loss.append(logs['loss'])\n",
    "        # self.val_loss.append(logs['val_loss'])\n",
    "        self.loss.append(logs.get(\"loss\")) ### \n",
    "        self.val_loss.append(logs.get(\"val_loss\")) ###  \n",
    "        # model 1 \n",
    "        #self.first_output_model1_loss[0].append(logs.get(\"first_output_model1_loss\")) \n",
    "        #self.second_output_model1_loss[0].append(logs.get(\"second_output_model1_loss\"))\n",
    "        self.first_output_model1_mse.append(logs.get(\"first_output_model1_mean_squared_error\")) \n",
    "        self.first_output_model1_mae.append(logs.get(\"first_output_model1_mean_absolute_error\"))\n",
    "        self.first_output_model1_mape.append(logs.get(\"first_output_model1_mean_absolute_percentage_error\"))\n",
    "        self.second_output_model1_mse.append(logs.get(\"second_output_model1_mean_squared_error\"))\n",
    "        self.second_output_model1_mae.append(logs.get(\"second_output_model1_mean_absolute_error\")) \n",
    "        self.second_output_model1_mape.append(logs.get(\"second_output_model1_mean_absolute_percentage_error\"))\n",
    "        \n",
    "        #self.val_first_output_model1_loss[0].append(logs.get(\"val_first_output_model1_loss\")) \n",
    "        #self.val_second_output_model1_loss[0].append(logs.get(\"val_second_output_model1_loss\")) \n",
    "        self.val_first_output_model1_mse.append(logs.get(\"val_first_output_model1_mean_squared_error\")) \n",
    "        self.val_first_output_model1_mae.append(logs.get(\"val_first_output_model1_mean_absolute_error\")) \n",
    "        self.val_first_output_model1_mape.append(logs.get(\"val_first_output_model1_mean_absolute_percentage_error\")) \n",
    "        self.val_second_output_model1_mse.append(logs.get(\"val_second_output_model1_mean_squared_error\")) \n",
    "        self.val_second_output_model1_mae.append(logs.get(\"val_second_output_model1_mean_absolute_error\")) \n",
    "        self.val_second_output_model1_mape.append(logs.get(\"val_second_output_model1_mean_absolute_percentage_error\")) \n",
    "        \n",
    "        # model 2 \n",
    "        #self.first_output_model2_loss[1].append(logs.get(\"first_output_model2_loss\")) \n",
    "        #self.second_output_model2_loss[1].append(logs.get(\"second_output_model2_loss\"))                               \n",
    "        self.first_output_model2_mse.append(logs.get(\"first_output_model2_mean_squared_error\"))  \n",
    "        self.first_output_model2_mae.append(logs.get(\"first_output_model2_mean_absolute_error\")) \n",
    "        self.first_output_model2_mape.append(logs.get(\"first_output_model2_mean_absolute_percentage_error\")) \n",
    "        self.second_output_model2_mse.append(logs.get(\"second_output_model2_mean_squared_error\")) \n",
    "        self.second_output_model2_mae.append(logs.get(\"second_output_model2_mean_absolute_error\")) \n",
    "        self.second_output_model2_mape.append(logs.get(\"second_output_model2_mean_absolute_percentage_error\")) \n",
    "        \n",
    "        #self.val_first_output_model2_loss[1].append(logs.get(\"val_first_output_model2_loss\")) \n",
    "        #self.val_second_output_model2_loss[1].append(logs.get(\"val_second_output_model2_loss\")) \n",
    "        self.val_first_output_model2_mse.append(logs.get(\"val_first_output_model2_mean_squared_error\")) \n",
    "        self.val_first_output_model2_mae.append(logs.get(\"val_first_output_model2_mean_absolute_error\")) \n",
    "        self.val_first_output_model2_mape.append(logs.get(\"val_first_output_model2_mean_absolute_percentage_error\")) \n",
    "        self.val_second_output_model2_mse.append(logs.get(\"val_second_output_model2_mean_squared_error\")) \n",
    "        self.val_second_output_model2_mae.append(logs.get(\"val_second_output_model2_mean_absolute_error\")) \n",
    "        self.val_second_output_model2_mape.append(logs.get(\"val_second_output_model2_mean_absolute_percentage_error\")) \n",
    "        \n",
    "        # model 3 \n",
    "        #self.first_output_model3_loss[2].append(logs.get(\"first_output_model3_loss\")) \n",
    "        #self.second_output_model3_loss[2].append(logs.get(\"second_output_model3_loss\"))                             \n",
    "        self.first_output_model3_mse.append(logs.get(\"first_output_model3_mean_squared_error\")) \n",
    "        self.first_output_model3_mae.append(logs.get(\"first_output_model3_mean_absolute_error\")) \n",
    "        self.first_output_model3_mape.append(logs.get(\"first_output_model3_mean_absolute_percentage_error\")) \n",
    "        self.second_output_model3_mse.append(logs.get(\"second_output_model3_mean_squared_error\")) \n",
    "        self.second_output_model3_mae.append(logs.get(\"second_output_model3_mean_absolute_error\")) \n",
    "        self.second_output_model3_mape.append(logs.get(\"second_output_model3_mean_absolute_percentage_error\"))\n",
    "        \n",
    "        #self.val_first_output_model3_loss[2].append(logs.get(\"val_first_output_model3_loss\")) \n",
    "        #self.val_second_output_model3_loss[2].append(logs.get(\"val_second_output_model3_loss\")) \n",
    "        self.val_first_output_model3_mse.append(logs.get(\"val_first_output_model3_mean_squared_error\")) \n",
    "        self.val_first_output_model3_mae.append(logs.get(\"val_first_output_model3_mean_absolute_error\")) \n",
    "        self.val_first_output_model3_mape.append(logs.get(\"val_first_output_model3_mean_absolute_percentage_error\")) \n",
    "        self.val_second_output_model3_mse.append(logs.get(\"val_second_output_model3_mean_squared_error\")) \n",
    "        self.val_second_output_model3_mae.append(logs.get(\"val_second_output_model3_mean_absolute_error\"))\n",
    "        self.val_second_output_model3_mape.append(logs.get(\"val_second_output_model3_mean_absolute_percentage_error\")) \n",
    "        \n",
    "        # model 4 \n",
    "        #self.first_output_model4_loss[3].append(logs.get(\"first_output_model4_loss\")) \n",
    "        #self.second_output_model4_loss[3].append(logs.get(\"second_output_model4_loss\"))                                 \n",
    "        self.first_output_model4_mse.append(logs.get(\"first_output_model4_mean_squared_error\")) \n",
    "        self.first_output_model4_mae.append(logs.get(\"first_output_model4_mean_absolute_error\")) \n",
    "        self.first_output_model4_mape.append(logs.get(\"first_output_model4_mean_absolute_percentage_error\")) \n",
    "        self.second_output_model4_mse.append(logs.get(\"second_output_model4_mean_squared_error\")) \n",
    "        self.second_output_model4_mae.append(logs.get(\"second_output_model4_mean_absolute_error\")) \n",
    "        self.second_output_model4_mape.append(logs.get(\"second_output_model4_mean_absolute_percentage_error\"))    \n",
    "        \n",
    "        #self.val_first_output_model4_loss[3].append(logs.get(\"val_first_output_model4_loss\")) \n",
    "        #self.val_second_output_model4_loss[3].append(logs.get(\"val_second_output_model4_loss\")) \n",
    "        self.val_first_output_model4_mse.append(logs.get(\"val_first_output_model4_mean_squared_error\")) \n",
    "        self.val_first_output_model4_mae.append(logs.get(\"val_first_output_model4_mean_absolute_error\")) \n",
    "        self.val_first_output_model4_mape.append(logs.get(\"val_first_output_model4_mean_absolute_percentage_error\")) \n",
    "        self.val_second_output_model4_mse.append(logs.get(\"val_second_output_model4_mean_squared_error\")) \n",
    "        self.val_second_output_model4_mae.append(logs.get(\"val_second_output_model4_mean_absolute_error\")) \n",
    "        self.val_second_output_model4_mape.append(logs.get(\"val_second_output_model4_mean_absolute_percentage_error\")) \n",
    "        \n",
    "# end of the class \n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, timesteps): #def create_dataset(dataset, timesteps=1):  -1\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-timesteps-1):\n",
    "        a = dataset[i:(i+timesteps), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + timesteps, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "# TRAIN of X-xais Bearing 3 \n",
    "t = '00_Train_2003.11.25.12.17_14.07_12sets.csv'\n",
    "df_train_1 = pandas.read_csv(t, header=None, sep='\t', usecols=[4], engine='python') # X-xais of bearing 3 \n",
    "dataset_train_1 = df_train_1.values  # \n",
    "dataset_train_1 = dataset_train_1.astype('float32')\n",
    "print(' dataset_train_1.shape == ', dataset_train_1.shape)\n",
    "\n",
    "# TRAIN of Y-xais Bearing 3 \n",
    "df_train_2 = pandas.read_csv(t, header=None, sep='\t', usecols=[5], engine='python') # Y-xais of bearing 3\n",
    "dataset_train_2 = df_train_2.values  # \n",
    "dataset_train_2 = dataset_train_2.astype('float32')\n",
    "print(' dataset_train_2.shape == ', dataset_train_2.shape)\n",
    "\n",
    "# Test of X-axis Bearing 3\n",
    "t1 = '01_Test_2003.11.25.14.17_16.07_12sets.csv' \n",
    "df_test_1 = pandas.read_csv(t1, header=None, sep='\t', usecols=[4], engine='python') # Test of X-axis Bearing 3\n",
    "dataset_test_1 = df_test_1.values  # Prediction Target(label) Data\n",
    "dataset_test_1 = dataset_test_1.astype('float32')\n",
    "print(' dataset_test_1.shape == ', dataset_test_1.shape)\n",
    "\n",
    "# Test of Y-axis Bearing 3  \n",
    "df_test_2 = pandas.read_csv(t1, header=None, sep='\t', usecols=[5], engine='python') # Test of Y-axis Bearing 3  \n",
    "dataset_test_2 = df_test_2.values  # Prediction Target(label) Data\n",
    "dataset_test_2 = dataset_test_2.astype('float32')\n",
    "print(' dataset_test_2.shape == ', dataset_test_2.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize the dataset ---------------- # 18.2.25. \n",
    "scaler1 = MinMaxScaler(feature_range=(0, 1))  \n",
    "dataset_train_1 = scaler1.fit_transform(dataset_train_1)\n",
    "\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 18.3.16.\n",
    "dataset_train_2 = scaler2.fit_transform(dataset_train_2)\n",
    "\n",
    "scaler3 = MinMaxScaler(feature_range=(0, 1)) \n",
    "dataset_test_1 = scaler3.fit_transform(dataset_test_1) # TEST of X-xais of bearing 1\n",
    "\n",
    "scaler4 = MinMaxScaler(feature_range=(0, 1)) \n",
    "dataset_test_2 = scaler4.fit_transform(dataset_test_2) # TEST of Y-xais of bearing 1\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print(' dataset_train_1.shape == ', dataset_train_1.shape)\n",
    "print(' dataset_train_2.shape == ', dataset_train_2.shape)\n",
    "\n",
    "print(' dataset_test_1.shape == ', dataset_test_1.shape) # dataset_test_1  \n",
    "print(' dataset_test_2.shape == ', dataset_test_2.shape) # dataset_test_2\n",
    "print('-----------------------------------------')\n",
    "dataset_train_1_mean = numpy.mean(dataset_train_1) # mean  \n",
    "print('mean of dataset_train_1 == ', dataset_train_1_mean)\n",
    "dataset_train_1_var = numpy.var(dataset_train_1)  # variance, The variance is the average of the squared deviations from the mean, i.e., var = mean(abs(x - x.mean())**2). \n",
    "print('variance of dataset_train_1 == ', dataset_train_1_var)\n",
    "dataset_train_1_std = numpy.std(dataset_train_1) # std, The standard deviation is the square root of the average of the squared deviations from the mean, i.e., std = sqrt(mean(abs(x - x.mean())**2)) \n",
    "print('standard deviation of dataset_train_1 == ', dataset_train_1_std)\n",
    "print('-----------------------------------------')\n",
    "dataset_train_2_mean = numpy.mean(dataset_train_2) # mean  \n",
    "print('mean of dataset_train_1 == ', dataset_train_2_mean)\n",
    "dataset_train_2_var = numpy.var(dataset_train_2)  # variance  \n",
    "print('variance of dataset_train_2 == ', dataset_train_2_var)\n",
    "dataset_train_2_std = numpy.std(dataset_train_2) # standard deviation \n",
    "print('standard deviation of dataset_train_2 == ', dataset_train_2_std)\n",
    "\n",
    "print('-----------------------------------------')\n",
    "dataset_test_1_mean = numpy.mean(dataset_test_1) # mean    ... # TEST of X-xaix of bearing 1\n",
    "print('mean of dataset_test_1 == ', dataset_test_1_mean)\n",
    "dataset_test_1_var = numpy.var(dataset_test_1)  # variance\n",
    "print('variance of dataset_test_1 == ', dataset_test_1_var)\n",
    "dataset_test_1_std = numpy.std(dataset_test_1) # standard deviation\n",
    "print('standard deviation of dataset_test_1 == ', dataset_test_1_std)\n",
    "print('-----------------------------------------')\n",
    "dataset_test_2_mean = numpy.mean(dataset_test_2) # mean    ... # TEST of Y-xaix of bearing 1\n",
    "print('mean of dataset_test_2 == ', dataset_test_2_mean)\n",
    "dataset_test_2_var = numpy.var(dataset_test_2)  # variance\n",
    "print('variance of dataset_test_2 == ', dataset_test_2_var)\n",
    "dataset_test_2_std = numpy.std(dataset_test_2) # standard deviation\n",
    "print('standard deviation of dataset_test_2 == ', dataset_test_2_std)\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# writing var, std to file \n",
    "file = open(title+\"var_std_train.txt\", 'w') \n",
    "file.write(\"\\n# variance, standard deviation of train and test sets \")\n",
    "file.write(\"\\n mean of dataset_train_1 == \")\n",
    "file.write(str(dataset_train_1_mean))  #write() argument must be str, not numpy.float32\n",
    "file.write(\"\\n variance of dataset_train_1 == \")\n",
    "file.write(str(dataset_train_1_var))\n",
    "file.write(\"\\n standard deviation of dataset_train_1 == \")\n",
    "file.write(str(dataset_train_1_std))\n",
    "file.write(\"\\n----------------------------------------\")\n",
    "file.write(\"\\n mean of dataset_train_1 == \")\n",
    "file.write(str(dataset_train_2_mean))\n",
    "file.write(\"\\n variance of dataset_train_2 == \")\n",
    "file.write(str(dataset_train_2_var))\n",
    "file.write(\"\\n standard deviation of dataset_train_2 == \")\n",
    "file.write(str(dataset_train_2_std))\n",
    "file.write(\"\\n-----------------------------------------\")\n",
    "\n",
    "file.write(\"\\n mean of dataset_test_1 == \")\n",
    "file.write(str(dataset_test_1_mean))        #  ...  or  X-xaix of bearing 1\n",
    "file.write(\"\\n variance of dataset_test_1 == \")\n",
    "file.write(str(dataset_test_1_var)) \n",
    "file.write(\"\\n standard deviation of dataset_test_1 == \")\n",
    "file.write(str(dataset_test_1_std))\n",
    "file.write(\"\\n-----------------------------------------\")\n",
    "file.write(\"\\n mean of dataset_test_2 == \")\n",
    "file.write(str(dataset_test_2_mean))        # ...  or Y-xaix of bearing 1\n",
    "file.write(\"\\n variance of dataset_test_2 == \")\n",
    "file.write(str(dataset_test_2_var)) \n",
    "file.write(\"\\n standard deviation of dataset_test_2 == \")\n",
    "file.write(str(dataset_test_2_std)) \n",
    "file.write(\"-----------------------------------------\")\n",
    "file.close() \n",
    "\n",
    "# split into train and test sets ----------------\n",
    "train_1_size = int(len(dataset_train_1)) \n",
    "train_2_size = int(len(dataset_train_2))\n",
    "\n",
    "test_1_size = int(len(dataset_test_1)) #  TEST of X-xaix of bearing 1\n",
    "test_2_size = int(len(dataset_test_2)) #  TEST of Y-xaix of bearing 1\n",
    "\n",
    "print('-----------------------------------------')\n",
    "train_1 = dataset_train_1[0:train_1_size,:]  \n",
    "train_2 = dataset_train_2[0:train_2_size,:]\n",
    "\n",
    "test_1 = dataset_test_1[0:test_1_size,:] # TEST of X-xaix of bearing 1\n",
    "test_2 = dataset_test_2[0:test_2_size,:] # TEST of Y-xaix of bearing 1\n",
    "\n",
    "print('length of train_1  == ', int(len(train_1)))\n",
    "print('length of train_2 == ', int(len(train_2))) \n",
    "\n",
    "print('length of test_1 == ', int(len(test_1))) # X-xaix of bearing 1\n",
    "print('length of test_2 == ', int(len(test_2))) # Y-xaix of bearing 1 \n",
    "print(' Lets start ~!!! ') \n",
    "print('-----------------------------------------')\n",
    "\n",
    "features = 1\n",
    "timesteps = 29 #  \n",
    "\n",
    "trainX_1, trainY_1 = create_dataset(train_1, timesteps) # Train X-axis Bearing 3 all dataset for shallow mosels \n",
    "trainX_2, trainY_2 = create_dataset(train_2, timesteps) # Train Y-axis Bearing 3 all dataset for shallow models  \n",
    "\n",
    "testX_1, testY_1 = create_dataset(test_1, timesteps) # TEST of X-xaix of bearing 1\n",
    "testX_2, testY_2 = create_dataset(test_2, timesteps) # TEST Y-xaix of bearing 1 \n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] \n",
    "trainX_1 = numpy.reshape(trainX_1, (trainX_1.shape[0], trainX_1.shape[1], 1))\n",
    "testX_1 = numpy.reshape(testX_1, (testX_1.shape[0], testX_1.shape[1], 1))\n",
    "\n",
    "trainX_2 = numpy.reshape(trainX_2, (trainX_2.shape[0], trainX_2.shape[1], 1))\n",
    "testX_2 = numpy.reshape(testX_2, (testX_2.shape[0], testX_2.shape[1], 1))\n",
    "print('-----------------------------------')  \n",
    "print(' train_X.shape = ', trainX_1.shape) # train_X.shape =  (245730, 29, 1)\n",
    "print(' train_Y.shape = ', trainY_1.shape) # train_Y.shape =  (245730,)\n",
    "print(' train_X.shape = ', trainX_2.shape) # train_X.shape =  (245730, 29, 1)\n",
    "print(' train_Y.shape = ', trainY_2.shape) # train_Y.shape =  (245730,)\n",
    "\n",
    "print(' test_X.shape == ', testX_1.shape)  #  test_X.shape ==  (245730, 29, 1)  feature \n",
    "print(' test_Y.shape == ', testY_1.shape)  #  test_Y.shape ==  (245730,)  target \n",
    "print(' test_X.shape == ', testX_2.shape)  #  test_X.shape ==  (245730, 29, 1) feature \n",
    "print(' test_Y.shape == ', testY_2.shape)  #  test_Y.shape ==  (245730,)  target \n",
    "print('-----------------------------------') \n",
    "# testX_1 : testX 1 feature \n",
    "# testY_1 : testX 1 target(label)\n",
    "\n",
    "# testX_2 : testX 2 feature \n",
    "# testY_2 : testx 2 target(label) \n",
    "\n",
    "######################################### \n",
    "#batch_size = 32\n",
    "\n",
    "model_input_1 = Input(shape=(timesteps, features), name='first_input')  # (29, 1)  X-aix of bearing 3 \n",
    "\n",
    "model_input_2 = Input(shape=(timesteps, features), name='second_input')  # (29, 1) y-aix of bearing 3 \n",
    "\n",
    "import keras.backend.tensorflow_backend as K \n",
    "with K.tf.device('/gpu:0'):   #cpu:/0\n",
    "     \n",
    "    def model1_def(model_input_1, model_input_2):  \n",
    "        \n",
    "        inputs1_model1 = model_input_1\n",
    "        \n",
    "        conv_11 = Conv1D(filters=128, kernel_size=1, strides=1)(inputs1_model1)\n",
    "        #conv_11 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs1_model1)#(inputs1) \n",
    "        \"\"\"conv_11_bn = BatchNormalization()(conv_11) # \n",
    "        conv_11_bn_act = Activation('relu')(conv_11_bn)\n",
    "        conv_11_out = Dropout(0.1)(conv_11_bn_act) # (29,128)\"\"\"\n",
    "                                \n",
    "        inputs2_model1 = model_input_2\n",
    "        \n",
    "        conv_21 = Conv1D(filters=128, kernel_size=1, strides=1)(inputs2_model1) \n",
    "        # conv_21 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs2_model1)#(inputs2) \n",
    "        \"\"\"conv_21_bn = BatchNormalization()(conv_21) # \n",
    "        conv_21_bn_act = Activation('relu')(conv_21_bn)\n",
    "        conv_21_out = Dropout(0.1)(conv_21_bn_act) # (29,128)\"\"\"\n",
    "        \n",
    "        concatenated = concatenate([conv_11, conv_21], axis=-1)  \n",
    "        \n",
    "        conv_31 = Conv1D(filters=128, kernel_size=1, strides=1)(concatenated) \n",
    "        #conv_31 = Conv1D(filters=32, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(concatenated) \n",
    "        \n",
    "        conv_32 = Conv1D(filters=64, kernel_size=1, strides=1)(conv_31) \n",
    "        #conv_32 = Conv1D(filters=128, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(conv_31) \n",
    "        \n",
    "        ls_11 = (LSTM(64, dropout=0.1, recurrent_dropout=0.1))(conv_32)\n",
    "        \n",
    "        # LAST POINT, Do Not residual connection at this last layer ~!\n",
    "        out_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(ls_11)  \n",
    "        \n",
    "        outputs1_model1 = Dense(1, activation='relu', name = 'first_output_model1')(out_1)  # name\n",
    "        outputs2_model1 = Dense(1, activation='relu', name = 'second_output_model1')(out_1) # name\n",
    "        \n",
    "        model1 = Model(inputs=[inputs1_model1, inputs2_model1], \n",
    "                       outputs=[outputs1_model1, outputs2_model1], name='model1_conv_shallow') # name  \n",
    "        \n",
    "        return model1   \n",
    "        \n",
    "   \n",
    "    def model2_def(model_input_1, model_input_2):  \n",
    "        \n",
    "        inputs1_model2 = model_input_1\n",
    "        \n",
    "        ls_11 = (LSTM(64, return_sequences=True, dropout=0.1))(inputs1_model2)    \n",
    "        # conv_11 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs1_model2)    \n",
    "        \"\"\"     conv_11_bn = BatchNormalization()(conv_11) \n",
    "        conv_11_bn_act = Activation('relu')(conv_11_bn)\n",
    "        conv_11_out = Dropout(0.1)(conv_11_bn_act) # (29,128)\"\"\"\n",
    "            \n",
    "        inputs2_model2 = model_input_2\n",
    "        \n",
    "        ls_21 = (LSTM(64, return_sequences=True, dropout=0.1))(inputs2_model2)#(inputs2) \n",
    "        #conv_21 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs2_model2)#(inputs2) \n",
    "        \"\"\"conv_21_bn = BatchNormalization()(conv_21) \n",
    "        conv_21_bn_act = Activation('relu')(conv_21_bn)\n",
    "        conv_21_out = Dropout(0.1)(conv_21_bn_act) # (29,128)\"\"\"\n",
    "            \n",
    "        concatenated = concatenate([ls_11, ls_21], axis=-1)  # 128 <== 64 + 64\n",
    "        \n",
    "        ls_31 = (LSTM(32, return_sequences=True, dropout=0.1))(concatenated)    \n",
    "        ls_32 = (LSTM(128, dropout=0.1))(ls_31) #last lstm layer, remove..., return_sequences=True. \n",
    "        #ls_31 = (LSTM(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(concatenated)    \n",
    "        #ls_32 = (LSTM(128, dropout=0.1, recurrent_dropout=0.1))(ls_31) #last lstm layer, remove..., return_sequences=True. \n",
    "\n",
    "        # LAST POINT, Do Not residual connection at this last layer ~!\n",
    "        out_1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(ls_32)  \n",
    "        \n",
    "        outputs1_model2 = Dense(1, activation='relu', name = 'first_output_model2')(out_1)  # name\n",
    "        outputs2_model2 = Dense(1, activation='relu', name = 'second_output_model2')(out_1) # name\n",
    "\n",
    "        model2 = Model(inputs=[inputs1_model2, inputs2_model2], \n",
    "                       outputs=[outputs1_model2, outputs2_model2], name='model2_lstm_shallow') # name \n",
    "\n",
    "        return model2       \n",
    "        \n",
    "\n",
    "    def model3_def(model_input_1, model_input_2):  \n",
    "        \n",
    "        inputs1_model3 = model_input_1\n",
    "        \n",
    "        conv_11 = Conv1D(filters=8, kernel_size=1, strides=1)(inputs1_model3) #(inputs1) \n",
    "        #conv_11 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs1_model3) #(inputs1) \n",
    "        \"\"\"   conv_11_bn = BatchNormalization()(conv_11) \n",
    "        conv_11_bn_act = Activation('relu')(conv_11_bn)\n",
    "        conv_11_out = Dropout(0.1)(conv_11_bn_act) # (29,128)\"\"\"\n",
    "            \n",
    "        inputs2_model3 = model_input_2\n",
    "        \n",
    "        conv_21 = Conv1D(filters=8, kernel_size=1, strides=1)(inputs2_model3) #(inputs2) \n",
    "        #conv_21 = Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(inputs2_model3) #(inputs2) \n",
    "        \"\"\"   conv_21_bn = BatchNormalization()(conv_21) \n",
    "        conv_21_bn_act = Activation('relu')(conv_21_bn)\n",
    "        conv_21_out = Dropout(0.1)(conv_21_bn_act) # (29,128)\"\"\" \n",
    "            \n",
    "        concatenated = concatenate([conv_11, conv_21], axis=-1)  # 16 = 8+8 \n",
    "        \n",
    "        conv_31 = Conv1D(filters=16, kernel_size=1, strides=1)(concatenated)  \n",
    "        conv_32 = Conv1D(filters=128, kernel_size=1, strides=1)(conv_31)  \n",
    "        conv_33 = Conv1D(filters=16, kernel_size=1, strides=1)(conv_32)  \n",
    "        #conv_31 = Conv1D(filters=32, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(concatenated) \n",
    "    \n",
    "        residual_1 = keras.layers.Add()([conv_33, concatenated])  # 16, 16 \n",
    "        \n",
    " \n",
    "        conv_34 = Conv1D(filters=16, kernel_size=1, strides=1)(residual_1) \n",
    "        #conv_34_out = Dropout(0.1)(conv_32)\n",
    "        conv_35 = Conv1D(filters=128, kernel_size=1, strides=1)(conv_34)   \n",
    "        #conv_32 = Conv1D(filters=128, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(conv_31) \n",
    "        conv_36 = Conv1D(filters=16, kernel_size=1, strides=1)(conv_35) \n",
    "        #conv_34_out = Dropout(0.1)(conv_32)\n",
    "\n",
    "        residual_2 = keras.layers.Add()([conv_36, residual_1])  # 16, 16\n",
    "        \n",
    "   \n",
    "        conv_37 = Conv1D(filters=16, kernel_size=1, strides=1)(residual_2) \n",
    "        #conv_34_out = Dropout(0.1)(conv_32)\n",
    "        conv_38 = Conv1D(filters=128, kernel_size=1, strides=1)(conv_37)   \n",
    "        #conv_32 = Conv1D(filters=128, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(conv_31) \n",
    "        conv_39 = Conv1D(filters=16, kernel_size=1, strides=1)(conv_38) \n",
    "        #conv_34_out = Dropout(0.1)(conv_32)\n",
    "        ls_11 = (LSTM(64, dropout=0.1, recurrent_dropout=0.1))(conv_39)\n",
    "        # LAST POINT, Do Not residual connection at this last layer ~!\n",
    "\n",
    "        out_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(ls_11) # regularizers\n",
    "        \n",
    "        outputs1_model3 = Dense(1, activation='relu', name = 'first_output_model3')(out_1)  # name\n",
    "        outputs2_model3 = Dense(1, activation='relu', name = 'second_output_model3')(out_1) # name\n",
    "\n",
    "        model3 = Model(inputs=[inputs1_model3, inputs2_model3], \n",
    "                       outputs=[outputs1_model3, outputs2_model3], name='model3_conv_deep') # name\n",
    "        \n",
    "        return model3  \n",
    "        \n",
    "        \n",
    "    def model4_def(model_input_1, model_input_2):  \n",
    "\n",
    "        inputs1_model4 = model_input_1\n",
    "\n",
    "        ls_11 = (LSTM(32, return_sequences=True))(inputs1_model4)#(inputs1) \n",
    "        #ls_11 = (LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(inputs1_model4)#(inputs1) \n",
    "        \"\"\"ls_11_bn = BatchNormalization()(ls_11) #     \n",
    "        ls_11_bn_act = Activation('relu')(ls_11_bn)\n",
    "        ls_11_out = Dropout(0.1)(ls_11_bn_act) # (29,64)\"\"\"\n",
    "        \n",
    "        inputs2_model4 = model_input_2\n",
    "        \n",
    "        ls_21 = (LSTM(32, return_sequences=True))(inputs2_model4)#(inputs2) \n",
    "        #ls_21 = (LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(inputs2_model4)#(inputs2) \n",
    "        \"\"\"ls_21_bn = BatchNormalization()(ls_21) #     \n",
    "        ls_21_bn_act = Activation('relu')(ls_21_bn)\n",
    "        ls_21_out = Dropout(0.1)(ls_21_bn_act) # (29,64)\"\"\"\n",
    "\n",
    "        concatenated = concatenate([ls_11, ls_21], axis=-1)  # 64 = 32+32 \n",
    "               \n",
    "        # 2~4th Layer --- 64 - 16 - 64\n",
    "        ls_31 = (LSTM(64, return_sequences=True, dropout=0.1))(concatenated)    \n",
    "        ls_32 = (LSTM(16, return_sequences=True, dropout=0.1))(ls_31) \n",
    "        ls_33 = (LSTM(64, return_sequences=True, dropout=0.1))(ls_32) \n",
    "        #ls_31 = (LSTM(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(concatenated)    \n",
    "        #ls_32 = (LSTM(128, dropout=0.1, recurrent_dropout=0.1))(ls_31) #last lstm layer, remove..., return_sequences=True. \n",
    "        \n",
    "        # ever 2~3layers \n",
    "        residual_1 = keras.layers.Add()([ls_33, concatenated])  # 16, 16 \n",
    "        \n",
    "        # 5~7th Layer --- 64 - 16 - 64\n",
    "        ls_34 = (LSTM(64, return_sequences=True, dropout=0.1))(residual_1)    \n",
    "        ls_35 = (LSTM(16, return_sequences=True, dropout=0.1))(ls_34) \n",
    "        ls_36 = (LSTM(64, return_sequences=True, dropout=0.1))(ls_35) \n",
    "        \n",
    "        # ever 2~3layers \n",
    "        residual_2 = keras.layers.Add()([ls_36, residual_1])  # 16, 16 \n",
    "        \n",
    "        # 8~10th Layer --- 64 - 16 - 64\n",
    "        ls_37 = (LSTM(64, return_sequences=True, dropout=0.1))(residual_2)   \n",
    "        ls_38 = (LSTM(16, return_sequences=True, dropout=0.1))(ls_37)   \n",
    "        ls_39 = (LSTM(64, dropout=0.1))(ls_38) #last lstm layer, remove..., return_sequences=True. \n",
    "\n",
    "        # =========== # LAST POINT, Do Not residual connection at this last layer ~!\n",
    "        # 4th Layer \n",
    "        out_1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(ls_39)   \n",
    "        \n",
    "        outputs1_model4 = Dense(1, activation='relu', name = 'first_output_model4')(out_1)  # name\n",
    "        outputs2_model4 = Dense(1, activation='relu', name = 'second_output_model4')(out_1) # name\n",
    "\n",
    "        model4 = Model(inputs=[inputs1_model4, inputs2_model4], \n",
    "                       outputs=[outputs1_model4, outputs2_model4], name = 'model4_lstm_deep') # name  \n",
    "          \n",
    "        return model4   \n",
    "    ####################################################\n",
    "    def compile_and_train(model, epochs):\n",
    "        \n",
    "        optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "        \n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse', 'mae', 'mape'])    \n",
    "\n",
    "        #epochs=10   \n",
    "        #batch_size = 32  \n",
    "        saveweightfile = title + model.name + '.{epoch:02d}-_{loss:.2f}.hdf5'  # title # head of file name \n",
    "        checkpoint = ModelCheckpoint(saveweightfile, monitor='loss', verbose=1,\n",
    "                                     save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "        reducelr = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience=1) # 1    \n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=3) # 4(3)    \n",
    "\n",
    "        csv_logger = CSVLogger(title + 'log.csv', append=True, separator='\\n')  # separator=';', separator='\\t'\n",
    "        \n",
    "        #history = LossHistory()\n",
    "        #history.init() \n",
    "        #print ('Total epochs :  ' + str(i+1) + '  of ' + str(epochs))  # i = 0,1,2...\n",
    "        model.fit([trainX_1, trainX_2], [trainY_1, trainY_2],\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=False,\n",
    "                  validation_data=([testX_1, testX_2], [testY_1, testY_2]), \n",
    "                  callbacks=[history, csv_logger, checkpoint, reducelr, earlystop]) \n",
    "        \n",
    "        savemodelfile = title + model.name + str(epochs) + '.h5' # title # head of file name \n",
    "        model.save(savemodelfile)\n",
    "        print('> Saved trained model : %s' % savemodelfile)\n",
    "        return history   #return \" Train complete.\\n\"\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------------\n",
    "    model1_model = model1_def(model_input_1, model_input_2) #  New model 1 ... Conv Shallow   \n",
    "    model2_model = model2_def(model_input_1, model_input_2) #  New model 2 ... LSTM Shallow\n",
    "    model3_model = model3_def(model_input_1, model_input_2) #  New model 3 ... ResNet of conv1d\n",
    "    model4_model = model4_def(model_input_1, model_input_2) #  New model 4 ... ResNet of LSTM \n",
    "        \n",
    "    history = LossHistory() # 2019.12.15.\n",
    "    history.init()          # 2019.12.15.\n",
    "         \n",
    "    _ = compile_and_train(model1_model, epochs) # return history\n",
    "    _ = compile_and_train(model2_model, epochs) # return history\n",
    "    \n",
    "    _ = compile_and_train(model3_model, epochs)  \n",
    "    _ = compile_and_train(model4_model, epochs)  \n",
    "        # print('_',_)  # output is _ <keras.callbacks.History object       \n",
    "\n",
    "\n",
    "    def ensemble(models, model_input_1, model_input_2):\n",
    "        out1 = [model.outputs[0] for model in models]  # model.output[] , model = keras.Model \n",
    "        out2 = [model.outputs[1] for model in models]\n",
    "        # for i in range(int(X.shape[0])) \n",
    "        print('--------------------------------')\n",
    "        print('out1... = ', out1)  # tensor 4 items out, [<tf.Tensor 'first_output_model1/Relu:0' shape=(?, 1) >, <tf.Tensor 'first_output_model2/Relu:0' shape=(?, 1) >, <tf.Tensor 'first_output_model3/Relu:0' shape=(?, 1) >, <tf.Tensor 'first_output_model4/Relu:0' shape=(?, 1) >]\n",
    "        print('--------------------------------')\n",
    "        print('out2... = ', out2)  # tensor 4 items out, [<tf.Tensor 'second_output_model1/Relu:0' shape=(?, 1) , <tf.Tensor 'second_output_model2/Relu:0' shape=(?, 1) >, <tf.Tensor 'second_output_model3/Relu:0' shape=(?, 1) >, <tf.Tensor 'second_output_model4/Relu:0' shape=(?, 1) >]\n",
    "        print('--------------------------------')\n",
    "        output1_avr = Average()(out1) \n",
    "        output2_avr = Average()(out2) \n",
    "        print('--------------------------------')\n",
    "        print('output1_var... = ', output1_avr) # tensor 1 item out, output1_var...= Tensor(\"average_1/truediv:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)  \n",
    "        print('--------------------------------')\n",
    "        print('output2_var... = ', output2_avr) # tensor 1 item out, output2_var...= Tensor(\"average_2/truediv:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
    "        print('--------------------------------')\n",
    "        model = Model(inputs=[model_input_1, model_input_2], \n",
    "                      outputs=[output1_avr, output2_avr], \n",
    "                      name='ensemble')       \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    models = [model1_model, model2_model, model3_model, model4_model] \n",
    "    ###  model 1 ~ 4  = Model(inputs=[inputs1_, inputs2_], outputs=[outputs1_, outputs2_] )\n",
    "    # models_group2 = [model3_model, model4_model]\n",
    "    # models_group3 = [model1_model, model2_model, model3_model]\n",
    "    # models_group4 = [model1_model, model2_model, model3_model, model4_model]\n",
    "\n",
    "    ensemble_model = ensemble(models, model_input_1, model_input_2)\n",
    "\n",
    "    \n",
    "    # make predictions,    \n",
    "    print(' Start Prediction ... ')   \n",
    "    print(' Train X_1 (bearing 3 x), Train X_2 (bearing 3 y) Predicting...') \n",
    "    trainPredict_1, trainPredict_2 = ensemble_model.predict([trainX_1, trainX_2], batch_size=batch_size, verbose = 1) \n",
    "    print(' Test X_1 (bearing 3 x), Test X_2 (bearing 3 y) Predicting...')  \n",
    "    testPredict_1, testPredict_2 = ensemble_model.predict([testX_1, testX_2], batch_size=batch_size, verbose = 1) \n",
    "    print(' Prediction complete ...')    \n",
    "    \n",
    "# --------------------------------------------------------------        \n",
    "print('trainPredict_1 = model.predict([trainX_1, ], ) =', trainPredict_1)\n",
    "print('trainPredict_1.shape = ', trainPredict_1.shape)\n",
    "print('testPredict_1 = model.predict([testX_1, ], ) = ', testPredict_1)\n",
    "print('testPredict_1.shape = ', testPredict_1.shape)\n",
    "# --------------------------------------------------------------  \n",
    "\n",
    "# invert predictions  \n",
    "trainPredict_1 = scaler1.inverse_transform(numpy.array(trainPredict_1)) #trainX_1 or  (numpy.array(trainPredict_1)) \n",
    "trainY_1 = scaler1.inverse_transform(numpy.array([trainY_1,])) #trainY_1 or (numpy.array([trainY_1,]))\n",
    "print('scaler.inverse_transform(numpy.array(trainPredic_1)) = ', trainPredict_1)\n",
    "print('scaler.inverse_transform(numpy.array([trainY_1])) = ', trainY_1)\n",
    "\n",
    "testPredict_1 = scaler3.inverse_transform(numpy.array(testPredict_1)) #testX_1  \n",
    "testY_1 = scaler3.inverse_transform(numpy.array([testY_1])) #testY_1 or (numpy.array([testY_1,]))\n",
    "print('scaler.inverse_transform(numpy.array(testPredic_1)) = ', testPredict_1)\n",
    "print('scaler.inverse_transform(numpy.array([testY_1])) = ', testY_1)\n",
    "\n",
    "trainPredict_2 = scaler2.inverse_transform(numpy.array(trainPredict_2)) #trainX_2\n",
    "trainY_2 = scaler2.inverse_transform(numpy.array([trainY_2])) #testY_2 or (numpy.array([testY_2]))\n",
    "print('scaler.inverse_transform(numpy.array(trainPredic_2)) = ', trainPredict_2)\n",
    "print('scaler.inverse_transform(numpy.array(trainY_2)) = ', trainY_2)\n",
    "\n",
    "testPredict_2 = scaler4.inverse_transform(numpy.array(testPredict_2)) #testX_2\n",
    "testY_2 = scaler4.inverse_transform(numpy.array([testY_2])) #testY_2 or (numpy.array([testY_2]))  \n",
    "print('scaler.inverse_transform(numpy.array(testPredic_2)) = ', testPredict_2)\n",
    "print('scaler.inverse_transform(numpy.array(testY_1)) = ', testY_2)\n",
    "        \n",
    "# calculate root mean squared error of prediction  \n",
    "\n",
    "print('=========================================')\n",
    "trainScore_1 = math.sqrt(mean_squared_error(trainY_1[0], trainPredict_1[:,0])) #trainY_1, trainX_1\n",
    "print('Train_1 Score: %.5f RMSE' % (trainScore_1))  \n",
    "\n",
    "testScore_1 = math.sqrt(mean_squared_error(testY_1[0], testPredict_1[:,0]))  #testY_1, testX_1\n",
    "print('Test_1 Score: %.5f RMSE' % (testScore_1))  \n",
    "print('-----------------------------------------')\n",
    "\n",
    "trainScore_2 = math.sqrt(mean_squared_error(trainY_2[0], trainPredict_2[:,0])) #trainY_2, trainX_2\n",
    "print('Train_2 Score: %.5f RMSE' % (trainScore_2))  \n",
    "\n",
    "testScore_2 = math.sqrt(mean_squared_error(testY_2[0], testPredict_2[:,0])) #testY_2, testX_2\n",
    "print('Test_2 Score: %.5f RMSE' % (testScore_2)) \n",
    "print('=========================================')\n",
    "\n",
    "trainScore_1_mse = mean_squared_error(trainY_1[0], trainPredict_1[:,0]) #trainY_1, trainX_1\n",
    "print('Train_1 Score: %.5f MSE' % (trainScore_1_mse))  \n",
    "\n",
    "testScore_1_mse = mean_squared_error(testY_1[0], testPredict_1[:,0])  #testY_1, testX_1\n",
    "print('Test_1 Score: %.5f MSE' % (testScore_1_mse))  \n",
    "print('-----------------------------------------')\n",
    "\n",
    "trainScore_2_mse = mean_squared_error(trainY_2[0], trainPredict_2[:,0]) #trainY_2, trainX_2\n",
    "print('Train_2 Score: %.5f MSE' % (trainScore_2_mse))  \n",
    "\n",
    "testScore_2_mse = mean_squared_error(testY_2[0], testPredict_2[:,0]) #testY_2, testX_2\n",
    "print('Test_2 Score: %.5f MSE' % (testScore_2_mse)) \n",
    "print('=========================================')\n",
    "\n",
    "trainScore_1_mae = mean_absolute_error(trainY_1[0], trainPredict_1[:,0]) #trainY_1, trainX_1\n",
    "print('Train_1 Score: %.5f MAE' % (trainScore_1_mae))  \n",
    "\n",
    "testScore_1_mae = mean_absolute_error(testY_1[0], testPredict_1[:,0])  #testY_1, testX_1\n",
    "print('Test_1 Score: %.5f MAE' % (testScore_1_mae))  \n",
    "print('-----------------------------------------')\n",
    "\n",
    "trainScore_2_mae = mean_absolute_error(trainY_2[0], trainPredict_2[:,0]) #trainY_2, trainX_2\n",
    "print('Train_2 Score: %.5f MAE' % (trainScore_2_mae))  \n",
    "\n",
    "testScore_2_mae = mean_absolute_error(testY_2[0], testPredict_2[:,0]) #testY_2, testX_2\n",
    "print('Test_2 Score: %.5f MAE' % (testScore_2_mae)) \n",
    "print('=========================================')\n",
    "\n",
    "\n",
    "# calculate mean, std, var of prediction  \n",
    "trainPredict_1_mean = numpy.mean(trainPredict_1) # mean  \n",
    "print('mean of trainPredict_1_mean == ', trainPredict_1_mean)\n",
    "trainPredict_1_var = numpy.var(trainPredict_1)  # variance  \n",
    "print('variance of trainPredict_1_var == ', trainPredict_1_var)\n",
    "trainPredict_1_std = numpy.std(trainPredict_1) # standard deviation \n",
    "print('standard deviation of trainPredict_1_std == ', trainPredict_1_std)\n",
    "print('-----------------------------------------')\n",
    "trainPredict_2_mean = numpy.mean(trainPredict_2) # mean  \n",
    "print('mean of trainPredict_2_mean == ', trainPredict_2_mean)\n",
    "trainPredict_2_var = numpy.var(trainPredict_2)  # variance  \n",
    "print('variance of trainPredict_2_var == ', trainPredict_2_var)\n",
    "trainPredict_2_std = numpy.std(trainPredict_2) # standard deviation \n",
    "print('standard deviation of trainPredict_2_std == ', trainPredict_2_std)\n",
    "print('-----------------------------------------')\n",
    "testPredict_1_mean = numpy.mean(testPredict_1) # mean  \n",
    "print('mean of testPredict_1_mean == ', testPredict_1_mean)\n",
    "testPredict_1_var = numpy.var(testPredict_1)  # variance  \n",
    "print('variance of testPredict_1_var == ', testPredict_1_var)\n",
    "testPredict_1_std = numpy.std(testPredict_1) # standard deviation \n",
    "print('standard deviation of testPredict_1_std == ', testPredict_1_std)\n",
    "print('-----------------------------------------')\n",
    "testPredict_2_mean = numpy.mean(testPredict_2) # mean  \n",
    "print('mean of testPredict_2_mean == ', testPredict_2_mean)\n",
    "testPredict_2_var = numpy.var(testPredict_2)  # variance  \n",
    "print('variance of testPredict_2_var == ', testPredict_2_var)\n",
    "testPredict_2_std = numpy.std(testPredict_2) # standard deviation \n",
    "print('standard deviation of testPredict_2_std == ', testPredict_2_std)\n",
    "print('=========================================')\n",
    "\n",
    "# writing var, std of prediction  \n",
    "file = open(title+\"var_std_Prediction.txt\", 'w') \n",
    "file.write(\" # calculate mean, std, var of prediction \\n\")\n",
    "file.write(\"mean of trainPredict_1_mean == \\n\")\n",
    "file.write(str(trainPredict_1_mean))\n",
    "file.write(\"variance of trainPredict_1_var == \\n\")\n",
    "file.write(str(trainPredict_1_var))\n",
    "file.write(\"standard deviation of trainPredict_1_std == \\n\")\n",
    "file.write(str(trainPredict_1_std))\n",
    "file.write(\"-----------------------------------------\\n\")\n",
    "file.write(\"mean of trainPredict_2_mean == \\n\")\n",
    "file.write(str(trainPredict_2_mean))\n",
    "file.write(\"variance of trainPredict_2_var == \\n\")\n",
    "file.write(str(trainPredict_2_var))\n",
    "file.write(\"standard deviation of trainPredict_2_std == \\n\")\n",
    "file.write(str(trainPredict_2_std))\n",
    "file.write(\"-----------------------------------------\\n\")\n",
    "file.write(\"mean of testPredict_1_mean == \\n\")\n",
    "file.write(str(testPredict_1_mean))\n",
    "file.write(\"variance of testPredict_1_var == \\n\")\n",
    "file.write(str(testPredict_1_var))\n",
    "file.write(\"standard deviation of testPredict_1_std == \\n\")\n",
    "file.write(str(testPredict_1_std))\n",
    "file.write(\"-----------------------------------------\\n\")\n",
    "file.write(\"mean of testPredict_2_mean == \\n\")\n",
    "file.write(str(testPredict_2_mean))\n",
    "file.write(\"variance of testPredict_2_var == \\n\")\n",
    "file.write(str(testPredict_2_var))\n",
    "file.write(\"standard deviation of testPredict_2_std == \\n\")\n",
    "file.write(str(testPredict_2_std))\n",
    "file.write(\"-----------------------------------------\\n\")\n",
    "file.close() \n",
    "\n",
    "# ---------------------------------\n",
    "# shift train predictions for plotting\n",
    "\n",
    "# --- trainX_1\n",
    "trainPredictPlot_1 = numpy.empty_like(dataset_train_1)# train_1 = dataset_train_1[0:train_1_size,:] \n",
    "trainPredictPlot_1[:, :] = numpy.nan  \n",
    "trainPredictPlot_1[timesteps:len(trainPredict_1)+timesteps, :] = trainPredict_1 #trainX_1\n",
    "# --- trainX_2 \n",
    "trainPredictPlot_2 = numpy.empty_like(dataset_train_2)# train_2 = dataset_train_2[0:train_2_size,:]\n",
    "trainPredictPlot_2[:, :] = numpy.nan\n",
    "trainPredictPlot_2[timesteps:len(trainPredict_2)+timesteps, :] = trainPredict_2 #trainX_1\n",
    "\n",
    "\n",
    "# shift test predictions for plotting \n",
    "# --- testX_1\n",
    "testPredictPlot_1 = numpy.empty_like(dataset_test_1)# test_1 = dataset_test_1[0:test_1_size,:]\n",
    "testPredictPlot_1[:, :] = numpy.nan\n",
    "testPredictPlot_1[timesteps:len(testPredict_1)+timesteps, :] = testPredict_1 #testX_1\n",
    "# --- testX_2\n",
    "testPredictPlot_2 = numpy.empty_like(dataset_test_2)# test_2 = dataset_test_2[0:test_2_size,:]\n",
    "testPredictPlot_2[:, :] = numpy.nan\n",
    "testPredictPlot_2[timesteps:len(testPredict_2)+timesteps, :] = testPredict_2 #testX_2\n",
    "\n",
    "# ---------------------------------\n",
    "# print to # Layer, Shape, Parameter \n",
    "print('model summary ... ', ensemble_model.summary())  # keras.models.model()\n",
    "# ---------------------------------\n",
    "\n",
    "s = title # head of file name \n",
    "# ---------------------------------\n",
    "# plot graph\n",
    "plot_model(ensemble_model, to_file=s+'model_image.png')  # from keras.utils import plot_model \n",
    "# ---------------------------------\n",
    "\n",
    "# --- trainX_1\n",
    "plt.plot(scaler1.inverse_transform(dataset_train_1), label='Groundtruth')\n",
    "plt.plot(trainPredictPlot_1, label='Training')\n",
    "plt.title('Train of Bearing3 X-axis from 12:17 to 14:07, 2hours (12sets)') \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('X-axis samples')\n",
    "plt.legend() \n",
    "plt.savefig(s+'Train_1_Xaxis.png') \n",
    "plt.show()\n",
    "# --- trainX_2\n",
    "plt.plot(scaler2.inverse_transform(dataset_train_2), label='Groundtruth')\n",
    "plt.plot(trainPredictPlot_2, label='Training')\n",
    "plt.title('Train of Bearing3 Y-axis from 12:17 to 14:07, 2hours (12sets)') \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Y-axis samples')\n",
    "plt.legend() \n",
    "plt.savefig(s+'Train_2_Yaxis.png') \n",
    "plt.show()\n",
    "\n",
    "# --- testX_1\n",
    "plt.plot(scaler3.inverse_transform(dataset_test_1), label='Groundtruth')\n",
    "plt.plot(testPredictPlot_1, label='Test')\n",
    "plt.title('Test of Bearing 3 X-axis from 14:17 to 16:07, 2hours')   ### TEST bearing 3 \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('X-axis samples')\n",
    "plt.legend() \n",
    "plt.savefig(s+'Test_1_Xaxis.png') \n",
    "plt.show()\n",
    "# --- testX_2\n",
    "plt.plot(scaler4.inverse_transform(dataset_test_2), label='Groundtruth')\n",
    "plt.plot(testPredictPlot_2, label='Test')\n",
    "plt.title('Test of Bearing 3 Y-axis from 14:17 to 16:07, 2hours')   ### TEST bearing 3 \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Y-axis samples')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(s+'Test_2_Yaxis.png')  \n",
    "plt.show()\n",
    "print('--------------------------------------------------------')   \n",
    "print('history.___train___loss ===', history.loss)   \n",
    "print('history.___test___loss ===', history.val_loss)\n",
    "\n",
    "# model 1    \n",
    "#print('history.first_output_model1_loss = ', history.first_output_model1_loss) \n",
    "#print('history.second_out_model1_loss = '  , history.second_output_model1_loss)\n",
    "\n",
    "print('history.first_output_model1_mse =', history.first_output_model1_mse) \n",
    "print('history.first_output_model1_mae =', history.first_output_model1_mae)\n",
    "print('history.first_output_model1_mape =', history.first_output_model1_mape) \n",
    "print('history.second_output_model1_mse =', history.second_output_model1_mse) \n",
    "print('histroy.second_output_model1_mae = ', history.second_output_model1_mae) \n",
    "print('history.second_output_model1_mape = ', history.second_output_model1_mape) \n",
    "        \n",
    "#print('history.val_first_output_model1_loss = ', history.val_first_output_model1_loss)\n",
    "#print('history.val_second_output_model1_loss = ', history.val_second_output_model1_loss) \n",
    "\n",
    "print('history.val_first_output_model1_mse = ', history.val_first_output_model1_mse)\n",
    "print('history.val_first_output_model1_mae = ', history.val_first_output_model1_mae) \n",
    "print('history.val_first_output_model1_mape =', history.val_first_output_model1_mape) \n",
    "print('history.val_second_output_model1_mse = ', history.val_second_output_model1_mse) \n",
    "print('history.val_second_output_model1_mae = ', history.val_second_output_model1_mae) \n",
    "print('history.val_second_output_model1_mape = ', history.val_second_output_model1_mape)\n",
    "print('--------------------------------------------------------')              \n",
    "\n",
    "# model 2 \n",
    "#print('history.first_output_model2_loss = ', history.first_output_model2_loss) \n",
    "#print('history.second_output_model2_loss = ', history.second_output_model2_loss)\n",
    "\n",
    "print('history.first_output_model2_se = ', history.first_output_model2_mse)\n",
    "print('history.first_output_model2_mae = ', history.first_output_model2_mae)\n",
    "print('history.first_output_model2_mape = ', history.first_output_model2_mape)\n",
    "print('history.second_output_model2_mse = ', history.second_output_model2_mse)\n",
    "print('history.second_output_model2_mae = ', history.second_output_model2_mae)\n",
    "print('history.second_output_model2_mape = ', history.second_output_model2_mape)\n",
    "\n",
    "#print('history.val_first_output_model2_loss = ', history.val_first_output_model2_loss)\n",
    "#print('history.val_second_output_model2_loss = ', history.val_second_output_model2_loss)\n",
    "\n",
    "print('history.val_first_output_model2_mse =', history.val_first_output_model2_mse)\n",
    "print('history.val_first_output_model2_mae = ', history.val_first_output_model2_mae)\n",
    "print('history.val_first_output_model2_mape =', history.val_first_output_model2_mape)\n",
    "print('history.val_second_output_model2_mse =', history.val_second_output_model2_mse)\n",
    "print('history.val_second_output_model2_mae = ', history.val_second_output_model2_mae)\n",
    "print('history.val_second_output_model2_mape = ', history.val_second_output_model2_mape)\n",
    "print('--------------------------------------------------------')              \n",
    "\n",
    "# model 3 \n",
    "#print('history.first_output_model3_loss = ', history.first_output_model3_loss) \n",
    "#print('history.second_output_model3_loss = ', history.second_output_model3_loss)\n",
    "\n",
    "print('history.first_output_model3_mse = ', history.first_output_model3_mse)\n",
    "print('history.first_output_model3_mae = ', history.first_output_model3_mae)\n",
    "print('history.first_output_model3_mape = ', history.first_output_model3_mape)\n",
    "print('history.second_output_model3_mse = ', history.second_output_model3_mse)\n",
    "print('history.second_output_model3_mae = ', history.second_output_model3_mae)\n",
    "print('history.second_output_model3_mape = ', history.second_output_model3_mape)\n",
    "        \n",
    "#print('history.val_first_output_model3_loss = ', history.val_first_output_model3_loss)\n",
    "#print('history.val_second_output_model3_loss = ', history.val_second_output_model3_loss)\n",
    "                                          \n",
    "print('history.val_first_output_model3_mse = ', history.val_first_output_model3_mse)\n",
    "print('history.val_first_output_model3_mae = ', history.val_first_output_model3_mae)\n",
    "print('history.val_first_output_model3_mape = ', history.val_first_output_model3_mape)\n",
    "print('history.val_second_output_model3_mse = ', history.val_second_output_model3_mse)\n",
    "print('history.val_second_output_model3_mae = ', history.val_second_output_model3_mae)\n",
    "print('history.val_second_output_model3_mape = ', history.val_second_output_model3_mape)\n",
    "print('--------------------------------------------------------')             \n",
    "\n",
    "# model 4 \n",
    "#print('history.first_output_model4_loss = ', history.first_output_model4_loss)\n",
    "#print('history.second_output_model4_loss = ', history.second_output_model4_loss)\n",
    "                                          \n",
    "print('history.first_output_model4_mse = ', history.first_output_model4_mse)\n",
    "print('history.first_output_model4_mae = ', history.first_output_model4_mae)\n",
    "print('history.first_output_model4_mape = ', history.first_output_model4_mape)\n",
    "print('history.second_output_model4_mse = ', history.second_output_model4_mse)\n",
    "print('history.second_output_model4_mae = ', history.second_output_model4_mae)\n",
    "print('history.second_output_model4_mape = ', history.second_output_model4_mape) \n",
    "            \n",
    "#print('history.val_first_output_model4_loss = ', history.val_first_output_model4_loss)\n",
    "#print('history.val_second_output_model4_loss = ', history.val_second_output_model4_loss)\n",
    "\n",
    "print('history.val_first_output_model4_mse = ', history.val_first_output_model4_mse)\n",
    "print('history.val_first_output_model4_mae = ', history.val_first_output_model4_mae)\n",
    "print('history.val_first_output_model4_mape = ', history.val_first_output_model4_mape)\n",
    "print('history.val_second_output_model4_mse = ', history.val_second_output_model4_mse)\n",
    "print('history.val_second_output_model4_mae = ', history.val_second_output_model4_mae)\n",
    "print('history.val_second_output_model4_mape = ', history.val_second_output_model4_mape)\n",
    "print('--------------------------------------------------------')  \n",
    "\n",
    "\n",
    "print('--------------------------------------------------------')  \n",
    "plt.plot(history.loss, 'bo', label='Training loss (' + str(str(format(history.loss[-1],'.5f'))+')')) # Train loss = MSE \n",
    "#plt.plot(history.val_loss, 'b', label='test loss') # Test loss = MSE \n",
    "plt.plot(history.val_loss, 'b', label='Test loss (' + str(str(format(history.val_loss[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Ensemble Model loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Squared Error)')\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_model_mse.png')\n",
    "plt.show()\n",
    "print('--------------------------------------------------------')  \n",
    "\n",
    "                                          \n",
    "                                          \n",
    "# loss (MSE) of model 1 \n",
    "plt.plot(history.first_output_model1_mse, ':', label='x-axis Train loss (' + str(str(format(history.first_output_model1_mse[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model1_mse, '--', label='x-axis Test loss (' + str(str(format(history.val_first_output_model1_mse[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model1_mse, '-.', label='y-axis Train loss (' + str(str(format(history.second_output_model1_mse[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model1_mse, '-', label='y-axis Test loss (' + str(str(format(history.val_second_output_model1_mse[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 1 Loss of bearing 3 train and bearing 3 test')       ### TEST bearing 3 \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Squared Error)')   #### Model 1 \n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MSE_Model_1.png')\n",
    "plt.show() \n",
    "\n",
    "# loss (MSE) of model 2 \n",
    "plt.plot(history.first_output_model2_mse, ':', label='x-axis Train loss (' + str(str(format(history.first_output_model2_mse[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model2_mse, '--', label='x-axis Test loss (' + str(str(format(history.val_first_output_model2_mse[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model2_mse, '-.', label='y-axis Train loss (' + str(str(format(history.second_output_model2_mse[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model2_mse, '-', label='y-axis Test loss (' + str(str(format(history.val_second_output_model2_mse[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 2 Loss of bearing 3 train and bearing 3 test')   ### TEST bearing 3     \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Squared Error)') #### Model 2 \n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MSE_Model_2.png')\n",
    "plt.show() \n",
    "\n",
    "# loss (MSE) of model 3 \n",
    "plt.plot(history.first_output_model3_mse, ':', label='x-axis Train loss (' + str(str(format(history.first_output_model3_mse[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model3_mse, '--', label='x-axis Test loss (' + str(str(format(history.val_first_output_model3_mse[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model3_mse, '-.', label='y-axis Train loss (' + str(str(format(history.second_output_model3_mse[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model3_mse, '-', label='y-axis Test loss (' + str(str(format(history.val_second_output_model3_mse[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 3 Loss of bearing 3 train and bearing 3 test') ### TEST bearing 3       \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Squared Error)')  #### Model 3 \n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MSE_Model_3.png')\n",
    "plt.show() \n",
    "      \n",
    "# loss (MSE) of model 4 \n",
    "plt.plot(history.first_output_model4_mse, ':', label='x-axis Train loss (' + str(str(format(history.first_output_model4_mse[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model4_mse, '--', label='x-axis Test loss (' + str(str(format(history.val_first_output_model4_mse[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model4_mse, '-.', label='y-axis Train loss (' + str(str(format(history.second_output_model4_mse[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model4_mse, '-', label='y-axis Test loss (' + str(str(format(history.val_second_output_model4_mse[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 4 Loss of bearing 3 train and bearing 3 test')   ### TEST bearing 3     \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Squared Error)')  #### Model 4 \n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MSE_Model_4.png')\n",
    "plt.show() \n",
    "\n",
    "print('--------------------------------------------------------')  \n",
    "      \n",
    "# MAE of model 1 \n",
    "plt.plot(history.first_output_model1_mae, ':', label='x-axis train MAE (' + str(str(format(history.first_output_model1_mae[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model1_mae, '--', label='x-axis test MAE (' + str(str(format(history.val_first_output_model1_mae[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model1_mae, '-.', label='y-axis train MAE (' + str(str(format(history.second_output_model1_mae[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model1_mae, '-', label='y-axis test MAE (' + str(str(format(history.val_second_output_model1_mae[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 1 Loss of bearing 3 train and bearing 3 test')   ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Error)')   ##### Model 1\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MAE_Model_1.png')\n",
    "plt.show() \n",
    "\n",
    "# MAE of model 2 \n",
    "plt.plot(history.first_output_model2_mae, ':', label='x-axis train MAE (' + str(str(format(history.first_output_model2_mae[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model2_mae, '--', label='x-axis test MAE (' + str(str(format(history.val_first_output_model2_mae[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model2_mae, '-.', label='y-axis train MAE (' + str(str(format(history.second_output_model2_mae[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model2_mae, '-', label='y-axis test MAE (' + str(str(format(history.val_second_output_model2_mae[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 2 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Error)')   ##### Model 2\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MAE_Model_2.png')\n",
    "plt.show() \n",
    "\n",
    "# MAE of model 3 \n",
    "plt.plot(history.first_output_model3_mae, ':', label='x-axis train MAE (' + str(str(format(history.first_output_model3_mae[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model3_mae, '--', label='x-axis test MAE (' + str(str(format(history.val_first_output_model3_mae[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model3_mae, '-.', label='y-axis train MAE (' + str(str(format(history.second_output_model3_mae[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model3_mae, '-', label='y-axis test MAE (' + str(str(format(history.val_second_output_model3_mae[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 3 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Error)')   ##### Model 3\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MAE_Model_3.png')\n",
    "plt.show() \n",
    "\n",
    "# MAE of model 4 \n",
    "plt.plot(history.first_output_model4_mae, ':', label='x-axis train MAE (' + str(str(format(history.first_output_model4_mae[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model4_mae, '--', label='x-axis test MAE (' + str(str(format(history.val_first_output_model4_mae[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model4_mae, '-.', label='y-axis train MAE (' + str(str(format(history.second_output_model4_mae[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model4_mae, '-', label='y-axis test MAE (' + str(str(format(history.val_second_output_model4_mae[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 4 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Error)')  ##### Model 4\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # ==> label=..., label=...\n",
    "plt.savefig(s+'Loss_MAE_Model_4.png')\n",
    "plt.show() \n",
    "      \n",
    "print('--------------------------------------------------------') \n",
    "      \n",
    "# MAPE of model 1 \n",
    "plt.plot(history.first_output_model1_mape, ':', label='x-axis train MAPE (' + str(str(format(history.first_output_model1_mape[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model1_mape, '--', label='x-axis test MAPE (' + str(str(format(history.val_first_output_model1_mape[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model1_mape, '-.', label='y-axis train MAPE (' + str(str(format(history.second_output_model1_mape[-31],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model1_mape, '-', label='y-axis test MAPE (' + str(str(format(history.val_second_output_model1_mape[-31],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 1 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Percentage Error)') ##### Model 1\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # => label=..., label=...\n",
    "plt.savefig(s+'Loss_MAPE_Modle_1.png')\n",
    "plt.show() \n",
    "\n",
    "# MAPE of model 2 \n",
    "plt.plot(history.first_output_model2_mape, ':', label='x-axis train MAPE (' + str(str(format(history.first_output_model2_mape[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model2_mape, '--', label='x-axis test MAPE (' + str(str(format(history.val_first_output_model2_mape[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model2_mape, '-.', label='y-axis train MAPE (' + str(str(format(history.second_output_model2_mape[-21],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model2_mape, '-', label='y-axis test MAPE (' + str(str(format(history.val_second_output_model2_mape[-21],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 2 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Percentage Error)') ##### Model 2\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # => label=..., label=...\n",
    "plt.savefig(s+'Loss_MAPE_Modle_2.png')\n",
    "plt.show() \n",
    "\n",
    "# MAPE of model 3 \n",
    "plt.plot(history.first_output_model3_mape, ':', label='x-axis train MAPE (' + str(str(format(history.first_output_model3_mape[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model3_mape, '--', label='x-axis test MAPE (' + str(str(format(history.val_first_output_model3_mape[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model3_mape, '-.', label='y-axis train MAPE (' + str(str(format(history.second_output_model3_mape[-11],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model3_mape, '-', label='y-axis test MAPE (' + str(str(format(history.val_second_output_model3_mape[-11],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 3 Loss of bearing 3 train and bearing 3 test') ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Percentage Error)') ##### Model 3\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # => label=..., label=...\n",
    "plt.savefig(s+'Loss_MAPE_Modle_3.png')\n",
    "plt.show() \n",
    "\n",
    "# MAPE of model 4 \n",
    "plt.plot(history.first_output_model4_mape, ':', label='x-axis train MAPE (' + str(str(format(history.first_output_model4_mape[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_first_output_model4_mape, '--', label='x-axis test MAPE (' + str(str(format(history.val_first_output_model4_mape[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.plot(history.second_output_model4_mape, '-.', label='y-axis train MAPE (' + str(str(format(history.second_output_model4_mape[-1],'.5f'))+')')) # Train loss = MSE \n",
    "plt.plot(history.val_second_output_model4_mape, '-', label='y-axis test MAPE (' + str(str(format(history.val_second_output_model4_mape[-1],'.5f'))+')')) # Test loss = MSE \n",
    "plt.title('Model 1 Loss of bearing 3 train and bearing 3 test')  ### TEST bearing 3\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Mean Absolute Percentage Error)') ##### Model 4\n",
    "plt.legend()  # plt.legend(['Train', 'Test']) # => label=..., label=...\n",
    "plt.savefig(s+'Loss_MAPE_Modle_4.png')\n",
    "plt.show()     \n",
    "print('--------------------------------------------------------') \n",
    "\n",
    "# ---------------- density plots \n",
    "plt.subplot(221)\n",
    "series1 = df_train_1 #df_train_1 = pandas.read_csv(t, header=None, sep='\t', usecols=[4], engine='python') \n",
    "series1.plot(kind='kde') \n",
    "plt.title(\"Density Plots of Train X-axis\")\n",
    "\n",
    "plt.subplot(222)\n",
    "series2 = df_train_2 #df_train_2 = pandas.read_csv(t, header=None, sep='\t', usecols=[5], engine='python')  \n",
    "series2.plot(kind='kde')\n",
    "plt.title(\"Density Plots of Train Y-axis\")\n",
    "\n",
    "plt.subplot(223)   # name 'df_test_1' is not defined \n",
    "series3 = df_test_1 #df_test_1 = pandas.read_csv(t1, header=None, sep='\t', usecols=[4], engine='python')\n",
    "series3.plot(kind='kde')\n",
    "plt.title(\"Density Plots of Test X-axis\")\n",
    "\n",
    "plt.subplot(224)\n",
    "series4 = df_test_2 #df_test_2 = pandas.read_csv(t1, header=None, sep='\t', usecols=[5], engine='python')\n",
    "series4.plot(kind='kde')\n",
    "plt.title(\"Density Plots of Test Y-axis\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(s+'densityplots.png') \n",
    "plt.show()\n",
    "\n",
    "# ---------------- Train Boxplot \n",
    "plt.subplot(221)\n",
    "df_train_x = pandas.DataFrame(df_train_1) #TrainX_1 dataset \n",
    "df_train_x.boxplot() \n",
    "plt.title(\"X-axis train target dataset\")\n",
    "\n",
    "plt.subplot(222)\n",
    "df_trainPredictPlot_1 = pandas.DataFrame(trainPredictPlot_1) #TrainX_1 Prediction\n",
    "df_trainPredictPlot_1.boxplot()  \n",
    "plt.title(\"X-axis train prediction\")\n",
    "\n",
    "plt.subplot(223)\n",
    "df_train_y = pandas.DataFrame(df_train_2) #TrainX_2 dataset\n",
    "df_train_y.boxplot() \n",
    "plt.title(\"Y-axis train target dataset\")\n",
    "\n",
    "plt.subplot(224)\n",
    "df_trainPredictPlot_2 = pandas.DataFrame(trainPredictPlot_2) #TrainX_2  Prediction\n",
    "df_trainPredictPlot_2.boxplot()  \n",
    "plt.title(\"Y-axis train prediction\")\n",
    "# -------\n",
    "plt.tight_layout()\n",
    "plt.savefig(s+'boxplot_train.png') \n",
    "plt.show()\n",
    "# ---------------- Test Boxplot \n",
    "plt.subplot(221) \n",
    "df_test_x = pandas.DataFrame(df_test_1) #Test dataset\n",
    "df_test_x.boxplot() \n",
    "plt.title(\"X-axis test target dataset\")\n",
    "\n",
    "plt.subplot(222)\n",
    "df_testPredictPlot_1 = pandas.DataFrame(testPredictPlot_1) # TestX_1  \n",
    "df_testPredictPlot_1.boxplot()\n",
    "plt.title(\"X-axis test prediction\")\n",
    "\n",
    "plt.subplot(223)\n",
    "df_test_y = pandas.DataFrame(df_test_2) #Test dataset\n",
    "df_test_y.boxplot() \n",
    "plt.title(\"Y-axis test target dataset\")\n",
    "\n",
    "plt.subplot(224)\n",
    "df_testPredictPlot_2 = pandas.DataFrame(testPredictPlot_2)  # TestX_2\n",
    "df_testPredictPlot_2.boxplot()\n",
    "plt.title(\"Y-axis test prediction\")\n",
    "# -------\n",
    "plt.tight_layout()\n",
    "plt.savefig(s+'boxplot_test.png') \n",
    "plt.show()\n",
    "# ---------------------------------\n",
    "print('------- Train_1_x_dataframe.describe() -------')\n",
    "print(df_train_x.describe())\n",
    "print('------- df_trainPredictPlot_1_x.describe() -------')\n",
    "print(df_trainPredictPlot_1.describe())\n",
    "print('------- Train_2_y_dataframe.describe() -------')\n",
    "print(df_train_x.describe())\n",
    "print('------- df_trainPredictPlot_2_y.describe() -------')  \n",
    "print(df_trainPredictPlot_2.describe())\n",
    "# ---------------------------------\n",
    "print('------- Test 1 x.describe() -------')\n",
    "print(df_test_x.describe())\n",
    "print('------- df_testPredictPlot_1_x.describe() -------')\n",
    "print(df_testPredictPlot_1.describe())\n",
    "print('------- Test 2 y.describe() -------')\n",
    "print(df_test_y.describe())\n",
    "print('------- df_testPredictPlot_2_y.describe() -------')\n",
    "print(df_testPredictPlot_2.describe())\n",
    "# ---------------------------------\n",
    "print('--- end of codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
